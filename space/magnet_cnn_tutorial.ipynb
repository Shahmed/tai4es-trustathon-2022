{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MagNet: Model the Geomagnetic Field\n",
    "\n",
    "\n",
    "This notebook describes a convolutional neural network approach to forecasting the disturbance storm time index. This is based on the second-place solution to a competition held by the National Oceanic and Atmospheric Association (NOAA) and National Aeronatics and Space Administration (NASA) in 2020-21 (https://www.drivendata.org/competitions/73/noaa-magnetic-forecasting/).\n",
    "\n",
    "In this notebook, you'll notice color-coded boxes, which may provide optional tasks, questions to discuss, or warnings. Here is the color-coding breakdown: \n",
    "* <span style=\"color:blue\">Blue Box</span> = Question to discuss\n",
    "* <span style=\"color:red\">Red Box</span> = Warning/Caution\n",
    "* <span style=\"color:gold\">Yellow Box</span> = Optional task to perform \n",
    "* <span style=\"color:green\">Green Box</span> = Useful Tip or Reminder\n",
    "\n",
    "## Background on Geospace Space Weather\n",
    "\n",
    "Just like the terrestrial weather we are used to experiencing in our daily lives, weather also occurs in the space environment. If you'd like a general primer on space weather and it's effects on the technological systems we rely on, check out [NASA's Space Place](https://spaceplace.nasa.gov/spaceweather/), as well as [NOAA's Space Weather Prediction Center (SWPC)](https://www.swpc.noaa.gov/), in particular their community dashboards.\n",
    "\n",
    "![HELIO_GRAPHIC_URL](https://ngdc.noaa.gov/geomag/img/challenge-banner.png \"HELIO\")\n",
    "\n",
    "## Background on the Geomagnetic Field\n",
    "\n",
    "The efficient transfer of energy from solar wind into the Earth’s magnetic field causes geomagnetic storms. The resulting variations in the magnetic field increase errors in magnetic navigation. The disturbance-storm-time index, or <i>Dst</i>, is a measure of the severity of the geomagnetic storm.\n",
    "\n",
    "As a key specification of the magnetospheric dynamics, the <i>Dst</i> index is used to drive geomagnetic disturbance models such as NOAA/NCEI’s High Definition Geomagnetic Model - Real-Time (HDGM-RT).\n",
    "![HDGMRT_GRAPHIC_URL](https://www.ngdc.noaa.gov/geomag/HDGM/images/HDGM-RT_2003_storm_720p.gif \"HDGM-RT\")\n",
    " \n",
    "In 2020-2021, NOAA and NASA conducted an international crowd sourced data science competition “MagNet: Model the Geomagnetic Field”:\n",
    "https://www.drivendata.org/competitions/73/noaa-magnetic-forecasting/\n",
    " \n",
    "Empirical models have been proposed as early as in 1975 to forecast <i>Dst</i> solely from solar-wind observations at the Lagrangian (L1) position by satellites such as NOAA’s Deep Space Climate Observatory (DSCOVR) or NASA's Advanced Composition Explorer (ACE). Over the past three decades, several models were proposed for solar wind forecasting of <i>Dst</i>, including empirical, physics-based, and machine learning approaches. While the ML models generally perform better than models based on the other approaches, there is still room to improve, especially when predicting extreme events. More importantly, we intentionally sought solutions that work on the raw, real-time data streams and are agnostic to sensor malfunctions and noise.\n",
    "\n",
    "## Modeling Task\n",
    "\n",
    "The MagNet competition task was to develop models for forecasting <i>Dst</i> that push the boundary of predictive performance, under operationally viable constraints, using the real-time solar-wind (RTSW) data feeds from NOAA’s DSCOVR and NASA’s ACE satellites. Improved models can provide more advanced warning of geomagnetic storms and reduce errors in magnetic navigation systems. Specifically, given one week of data ending at t minus 1 minute, the model must forecast <i>Dst</i> at time t and t plus one hour.\n",
    "\n",
    "The model described in this notebook is the benchmark model provided by the MagNet competition organizers. Long Short Term Memory networks or LSTMs are a special kind of recurrent neural network especially suited to time series data. In the related notebook, we will show you how to implement a first-pass LSTM model for predicting <i>Dst</i>.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question: </b> Can you describe the physical process between solar wind and ground geomagnetic disturbances? What is the <i>Dst</i> index primarily used for?\n",
    "Roughly 85% of the time, near Earth is geomagnetically quiet. How might these infrequent solar wind events make modeling their predicted effects challenging? How might you make an accurate model with very few extreme events/samples?\n",
    "</div>\n",
    "\n",
    "## Data Notes\n",
    "\n",
    "The target <i>Dst</i> values are measured by 4 ground-based observatories near the equator. These values are then averaged to provide a measurement of <i>Dst</i> for any given hour.\n",
    "To ensure similar distributions between the training and test data, the data is separated into three non-contiguous periods. All data are provided with a `period` and `timedelta` multi-index which indicates the relative timestep for each observation within a period, but not the real timestamp. The period identifiers and timedeltas are common across datasets. Converting back from our index date and time to real geophysical date and time as simple as adding the the start date/time in the table below to the relative timestep provided with the data.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\">Table: Dataset Period Time Ranges</div>\n",
    "\n",
    "| Period  | Beginning               | End                      |\n",
    "|---------|-------------------------|--------------------------|\n",
    "| train_a | 1998, 2, 16, '00:00:00' |  2001, 5, 31, '23:00:00' |\n",
    "| train_b |  2013, 6, 1, '00:00:00' |  2017, 7, 31, '23:00:00' |\n",
    "| train_c | 2004, 10, 1, '00:00:00' |  2010, 1, 31, '23:00:00' |\n",
    "|  test_a |  2001, 6, 1, '00:00:00' |  2004, 9, 30, '23:00:00' |\n",
    "|  test_b |  2010, 2, 1, '00:00:00' |  2013, 5, 31, '23:00:00' |\n",
    "|  test_c |  2017, 8, 1, '00:00:00' | 2020, 10, 31, '23:00:00' |\n",
    "\n",
    "\n",
    "![DATA_SPLITS_GRAPHIC_URL](https://github.com/ai2es/tai4es-trustathon-2022/tree/space/space/notebook_figures/Figure_Manoj_Activity_and_Training_Splits.png \"DATA_SPLITS\")\n",
    "<i>Figure: Plot shows solar activity as the sunspot number (SSN) (orange), the geomagnetic storm index Dst (blue), and the public and private data segments (red shaded).  SSN and Dst have been normalized. The time range shown is January 1998 through December 2022. This figure and the table preceeding it have been adapted from Neir et al. (manuscript in preparation).</i>\n",
    "\n",
    "The competitors used the training part (“train_a”,”train_b” and “train_c”) data to develop and improve their models. When they submitted a model, the competition platform used the test data sets (“test_a”,”test_b” and “test_c”)  to calculate the accuracy of the model. The model evaluation was done separately for a public leaderboard and for a private leaderboard. The public leaderboard was openly accessible whereas the private leaderboard was restricted to the competition administrators. The  data from all of the training sets (a, b, and c) were used on the public leaderboard and private leaderboard. We randomly sampled rows to be included in the public and private leaderboard. Based on relative performance from the public leaderboard as a clue, the teams iterated their models. The final ranking of the models was done on the private leaderboard.\n",
    "\n",
    "### Input (feature) data sources - there are 3:\n",
    "* Satellite measurements of the solar wind, including direction, speed, density and temperature, at 1-minute cadence.\n",
    "* Position of the satellite used for solar wind measurements. The ACE and DSCOVR satellites are positioned just outside Earth's exosphere approximately 1% of the distance from Earth to Sun. As noted above, this is referred to as the Sun Earth L1 position.\n",
    "* Number of sunspots on the Sun, measured monthly.\n",
    "\n",
    "Here is a description of several of these inputs (features) observed by the ACE or DSCOVR satellites (see [here](https://www.drivendata.org/competitions/73/noaa-magnetic-forecasting/page/279/) for the full list):\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "* bt - Interplanetary-magnetic-field magnitude (nT)\n",
    "* bx_gsm - Interplanetary-magnetic-field X-component in geocentric solar magnetospheric (GSM) coordinate (nT)\n",
    "* by_gsm - Interplanetary-magnetic-field Y-component in GSM coordinate (nT)\n",
    "* bz_gsm - Interplanetary-magnetic-field Z-component in (GSM) coordinate (nT)\n",
    "* density - Solar wind proton density (N/cm^3)\n",
    "* speed - Solar wind bulk speed (km/s) flowing from Sun to Earth\n",
    "* temperature - Solar wind ion temperature (Kelvin)\n",
    "</div>\n",
    "\n",
    "To get a feeling for the GSM coordinate reference frame:\n",
    "\n",
    "The X-axis is oriented from the Earth to the Sun. The positive Z-axis is chosen to be in the same sense as the northern magnetic pole. And the Y-axis is defined to be perpendicular to the Earth's magnetic dipole so that the X-Z plane contains the dipole axis. For additional details, see [here](https://www.spenvis.oma.be/help/background/coortran/coortran.html#GSM).\n",
    "\n",
    "To see how several of these parameter look during an example space weather event see [Figure 5](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2018SW001897#swe20716-fig-0005) of Redmon et al., 2018.\n",
    "\n",
    "## Additional code\n",
    "\n",
    "The code used here is available at: https://github.com/liyo6397/MagNet/. This repository also contains other notebooks and code for defining different model achitectures. The code is designed to be modular, so other architectures can easily be swapped for the one used here.\n",
    "\n",
    "## Contact\n",
    "Manoj.C.Nair@noaa.gov\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports\n",
    "\n",
    "The model is built with tensorflow, using the keras framework. We also use some common data science packages, including `numpy` and `pandas`, to prepare the data, and `matplotlib` for visualisation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import random\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download the data\n",
    "\n",
    "All the competition data is now publically available. ``public.zip`` is the data that was provided for training in the competition. ``private.zip`` is the unseen data used for scoring. The private data was divided into 2 parts: some was used for a public leaderboard visible during the competition, and the remainder was used for the final ranking.\n",
    "\n",
    "We will save the public and private data in separate directories, and use the public data for training and the private for validation, as in the competition."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "#download the data\n",
    "if [ ! -d \"data\" ]; then\n",
    "  wget https://ngdc.noaa.gov/geomag/data/geomag/magnet/public.zip\n",
    "  unzip public.zip\n",
    "  wget https://ngdc.noaa.gov/geomag/data/geomag/magnet/private.zip\n",
    "  unzip private.zip\n",
    "  mkdir data\n",
    "  mv public data\n",
    "  mv private data\n",
    "fi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load dataframes from csv files\n",
    "data_folder = \"data\"\n",
    "solar_cols = [\"period\", \"timedelta\", \"bx_gsm\", \"by_gsm\", \"bz_gsm\", \"bt\", \"speed\", \"density\", \"temperature\"]\n",
    "solar_train = pd.read_csv(os.path.join(data_folder, \"public\", \"solar_wind.csv\"), usecols=solar_cols, dtype={\"period\": \"category\"})\n",
    "dst_train = pd.read_csv(os.path.join(data_folder, \"public\", \"dst_labels.csv\"), dtype={\"period\": \"category\"})\n",
    "sunspots_train = pd.read_csv(os.path.join(data_folder, \"public\", \"sunspots.csv\"), dtype={\"period\": \"category\"})\n",
    "solar_test = pd.read_csv(os.path.join(data_folder, \"private\", \"solar_wind.csv\"), usecols=solar_cols, dtype={\"period\": \"category\"})\n",
    "dst_test = pd.read_csv(os.path.join(data_folder, \"private\", \"dst_labels.csv\"), dtype={\"period\": \"category\"})\n",
    "sunspots_test = pd.read_csv(os.path.join(data_folder, \"private\", \"sunspots.csv\"), dtype={\"period\": \"category\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploring the input data\n",
    "\n",
    "The variables have very long-tailed distributions.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Distribution of features "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot distributions\n",
    "fig, axs = plt.subplots(4, 2, figsize=(20, 12))\n",
    "for i, var in enumerate([\"bx_gsm\", \"by_gsm\", \"bz_gsm\", \"bt\", \"speed\", \"density\", \"temperature\", \"smoothed_ssn\"]):\n",
    "  plt.sca(axs.flat[i])\n",
    "  if var == \"smoothed_ssn\":\n",
    "    sunspots_train[\"smoothed_ssn\"].hist(bins=50)\n",
    "  else:\n",
    "    solar_train[var].hist(bins=50)\n",
    "  axs.flat[i].set_title(var)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Correlation\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> For merge the pandas Dataframe, users can assign the approach to merge: \n",
    "left, right, outter, inner, cross. The approach is similar to the \"join\" function in SQL. In the following example, \"left\" means to preserve the key order and perform left outter join. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>Correlation matrix:</b>\n",
    "Note that this is a slow command (several minutes) unless you have a GPU or TPU equivalent processor (then it's ~1 min).\n",
    "Take advantage of Pandas DataFrame and merge our Input (Feature) and Output (Label) data.\n",
    "i.e. merge, Solar Wind + Sunspots + Satellite Location + <i>Dst</i></div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "solar_sunspots = solar_train.merge(sunspots_train, how='left')\n",
    "corr = solar_sunspots.merge(dst_train, how='left').fillna(method=\"ffill\").corr()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.matshow(corr, cmap='seismic', vmin=-1, vmax=1, fignum=1)\n",
    "plt.xticks(range(corr.shape[1]), corr.columns, rotation=90)\n",
    "plt.gca().xaxis.tick_bottom()\n",
    "plt.yticks(range(corr.shape[1]), corr.columns)\n",
    "\n",
    "\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title(\"Feature Correlation Heatmap\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "del solar_sunspots\n",
    "del corr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot correlations with dst\n",
    "fig, axs = plt.subplots(4, 2, figsize=(20, 20))\n",
    "for i, var in enumerate([\"bx_gsm\", \"by_gsm\", \"bz_gsm\", \"bt\", \"speed\", \"density\", \"temperature\", \"smoothed_ssn\"]):\n",
    "  if var == \"smoothed_ssn\":\n",
    "    df = dst_train.merge(sunspots_train[[var,\"period\", \"timedelta\"]], 'left', on=[\"period\", \"timedelta\"])\n",
    "  else:\n",
    "    df = dst_train.merge(solar_train[[var,\"period\", \"timedelta\"]], 'left', on=[\"period\", \"timedelta\"])\n",
    "  axs.flat[i].scatter(df[var].values, df['dst'], s=2)\n",
    "  axs.flat[i].set_ylabel('dst')\n",
    "  axs.flat[i].set_xlabel(var)\n",
    "del df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing\n",
    "\n",
    "The model uses input data in the \n",
    "$(x, y, z)$ coordinate system, rather than the angular coordinate systems, so that the mean and standard deviation can be easily calculated. We use the GSM co-ordinate system, rather than GSE, since we found this gives better performance (see https://www.mssl.ucl.ac.uk/grid/iau/extra/local_copy/SP_coords/geo_sys.htm for details on these coordinate systems).\n",
    "\n",
    "Periods where the temperature data is $< 1$ are excluded from the training data, since this is not physically realistic and suggests that sensors are malfunctioning. Missing data is filled \n",
    "by linear interpolation (to reduce noise, the interpolation uses a smoothed rolling average, \n",
    "rather than just the 2 points immediately\n",
    "before and after the missing part).\n",
    "\n",
    "Data is normalized by subtracting the median and dividing by the inter-quartile range (this approach is used rather \n",
    "than the more usual mean and standard deviation because some variables have asymmetric distributions with \n",
    "long tails).\n",
    "\n",
    "To reduce the volume of data and the training time, data is aggregated  in 10-minute increments, taking the mean and standard deviation of each feature in \n",
    "the increment. This could alternatively be done as the first layer of the neural network, but it's more efficient \n",
    "to do it as a preprocessing step.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-success\"> Prepare data for training or prediction, returning 10-minute aggregates.Aggregate solar_data into 10-minute intervals and calculate the mean and standard deviation. Merge with sunspot data. Normalize the training data and save the scaling parameters in a dataframe (these are needed to transform data for prediction). </div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-warning\"> Optional Task: This method modifies the input dataframes solar, sunspots, and dst; if you want to keep the original dataframes, pass copies, e.g. prepare_data_1_min(solar.copy(), sunspots.copy(), dst.copy()). </div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function to preprocess data; we will use this to prepare the data for training \n",
    "# and validation\n",
    "\n",
    "def prepare_data_1_min(\n",
    "    solar: pd.DataFrame,\n",
    "    sunspots: Union[pd.DataFrame, float],\n",
    "    dst: pd.DataFrame = None,\n",
    "    norm_df=None,\n",
    "    output_folder: str = None,\n",
    "    coord_system: str = \"gsm\",\n",
    "    output_freq: str = \"10_minute\",\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    If ``dst`` is ``None``, prepare dataframe of feature variables only for prediction\n",
    "    using previously-calculated normalization scaling factors in ``norm_df``.\n",
    "    If ``dst`` is not ``None``, prepare dataframe of feature variables and labels for\n",
    "    model training. Calculate normalization scaling factors and\n",
    "    save in ``output_folder``. In this case ``output_folder`` must not be ``None``.\n",
    "    \n",
    "    Args:\n",
    "        solar: DataFrame containing solar wind data. This function uses the GSM\n",
    "            co-ordinates.\n",
    "        sunspots: DataFrame containing sunspots data, or float. If dataframe, will be\n",
    "            merged with solar data using timestamp. If float, all rows of output data\n",
    "            will use this number.\n",
    "        dst: ``None``, or DataFrame containing the disturbance storm time (Dst) data,\n",
    "        i.e. the labels for training\n",
    "        norm_df: ``None``, or DataFrame containing the normalization scaling factors to\n",
    "            apply\n",
    "        output_folder: Path to the directory where normalisation dataframe will be saved\n",
    "        coord_system: either \"gsm\" or \"gse\"\n",
    "        output_freq: \"10_minute\" or \"hour\", how to aggregate the output data\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        solar: DataFrame containing processed data and labels\n",
    "        train_cols: list of training columns\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "      os.mkdir(output_folder)\n",
    "\n",
    "    # convert timedelta\n",
    "    solar[\"timedelta\"] = pd.to_timedelta(solar[\"timedelta\"])\n",
    "\n",
    "    # merge data\n",
    "    solar[\"days\"] = solar[\"timedelta\"].dt.days\n",
    "    if isinstance(sunspots, pd.DataFrame):\n",
    "        sunspots[\"timedelta\"] = pd.to_timedelta(sunspots[\"timedelta\"])\n",
    "        sunspots.sort_values([\"period\", \"timedelta\"], inplace=True)\n",
    "        sunspots[\"month\"] = list(range(len(sunspots)))\n",
    "        sunspots[\"month\"] = sunspots[\"month\"].astype(int)\n",
    "        sunspots[\"days\"] = sunspots[\"timedelta\"].dt.days\n",
    "        solar = pd.merge(\n",
    "            solar,\n",
    "            sunspots[[\"period\", \"days\", \"smoothed_ssn\", \"month\"]],\n",
    "            \"left\",\n",
    "            [\"period\", \"days\"],\n",
    "        )\n",
    "    else:\n",
    "        solar[\"smoothed_ssn\"] = sunspots\n",
    "    solar.drop(columns=\"days\", inplace=True)\n",
    "    if dst is not None:\n",
    "        dst[\"timedelta\"] = pd.to_timedelta(dst[\"timedelta\"])\n",
    "        solar = pd.merge(solar, dst, \"left\", [\"period\", \"timedelta\"])\n",
    "    solar.sort_values([\"period\", \"timedelta\"], inplace=True)\n",
    "    solar.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # remove anomalous data (exclude from training and fill for prediction)\n",
    "    solar[\"bad_data\"] = False\n",
    "    solar.loc[solar[\"temperature\"] < 1, \"bad_data\"] = True\n",
    "    solar.loc[solar[\"temperature\"] < 1, [\"temperature\", \"speed\", \"density\"]] = np.nan\n",
    "    for p in solar[\"period\"].unique():\n",
    "        curr_period = solar[\"period\"] == p\n",
    "        solar.loc[curr_period, \"train_exclude\"] = (\n",
    "            solar.loc[curr_period, \"bad_data\"].rolling(60 * 24 * 7, center=False).max()\n",
    "        )\n",
    "\n",
    "    # fill missing data\n",
    "    if \"month\" in solar.columns:\n",
    "        solar[\"month\"] = solar[\"month\"].fillna(method=\"ffill\")\n",
    "    if coord_system == \"gsm\":\n",
    "        train_cols = [\n",
    "            \"bt\",\n",
    "            \"density\",\n",
    "            \"speed\",\n",
    "            \"bx_gsm\",\n",
    "            \"by_gsm\",\n",
    "            \"bz_gsm\",\n",
    "            \"smoothed_ssn\",\n",
    "        ]\n",
    "    elif coord_system == \"gse\":\n",
    "        train_cols = [\n",
    "            \"bt\",\n",
    "            \"density\",\n",
    "            \"speed\",\n",
    "            \"bx_gse\",\n",
    "            \"by_gse\",\n",
    "            \"bz_gse\",\n",
    "            \"smoothed_ssn\",\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid coord system {coord_system}\")\n",
    "    train_short = [c for c in train_cols if c != \"smoothed_ssn\"]\n",
    "    for p in solar[\"period\"].unique():\n",
    "        curr_period = solar[\"period\"] == p\n",
    "        solar.loc[curr_period, \"smoothed_ssn\"] = (\n",
    "            solar.loc[curr_period, \"smoothed_ssn\"]\n",
    "            .fillna(method=\"ffill\", axis=0)\n",
    "            .fillna(method=\"bfill\", axis=0)\n",
    "        )\n",
    "        # fill short gaps with interpolation\n",
    "        roll = (\n",
    "            solar[train_short]\n",
    "            .rolling(window=20, min_periods=5)\n",
    "            .mean()\n",
    "            .interpolate(\"linear\", axis=0, limit=60)\n",
    "        )\n",
    "        solar.loc[curr_period, train_short] = solar.loc[\n",
    "            curr_period, train_short\n",
    "        ].fillna(roll)\n",
    "        solar.loc[curr_period, train_short] = solar.loc[\n",
    "            curr_period, train_short\n",
    "        ].fillna(solar.loc[curr_period, train_short].mean(), axis=0)\n",
    "\n",
    "    # normalize data using median and inter-quartile range\n",
    "    if norm_df is None:\n",
    "        norm_df = solar[train_cols].median().to_frame(\"median\")\n",
    "        norm_df[\"lq\"] = solar[train_cols].quantile(0.25)\n",
    "        norm_df[\"uq\"] = solar[train_cols].quantile(0.75)\n",
    "        norm_df[\"iqr\"] = norm_df[\"uq\"] - norm_df[\"lq\"]\n",
    "    if output_folder is not None:\n",
    "        norm_df.to_csv(os.path.join(output_folder, \"norm_df.csv\"))\n",
    "    solar[train_cols] = (solar[train_cols] - norm_df[\"median\"]) / norm_df[\"iqr\"]\n",
    "\n",
    "    if dst is not None:\n",
    "        # interpolate target and shift target since we only have data up to t - 1 minute\n",
    "        solar[\"target\"] = (\n",
    "            solar[\"dst\"].shift(-1).interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "        )\n",
    "        # shift target for training t + 1 hour model\n",
    "        solar[\"target_shift\"] = solar[\"target\"].shift(-60)\n",
    "        solar[\"target_shift\"] = solar[\"target_shift\"].fillna(method=\"ffill\")\n",
    "        assert solar[train_cols + [\"target\", \"target_shift\"]].isnull().sum().sum() == 0\n",
    "\n",
    "    # aggregate features\n",
    "    if output_freq == \"10_minute\":\n",
    "        win = 10\n",
    "    elif output_freq == \"hour\":\n",
    "        win = 60\n",
    "    else:\n",
    "        raise ValueError(\"output_freq must be 10_minute or hour.\")\n",
    "    new_cols = [c + suffix for suffix in [\"_mean\", \"_std\"] for c in train_short]\n",
    "    train_cols = new_cols + [\"smoothed_ssn\"]\n",
    "    new_df = pd.DataFrame(index=solar.index, columns=new_cols)\n",
    "    for p in solar[\"period\"].unique():\n",
    "        curr_period = solar[\"period\"] == p\n",
    "        new_df.loc[curr_period] = (\n",
    "            solar.loc[curr_period, train_short]\n",
    "            .rolling(window=win, min_periods=1, center=False)\n",
    "            .agg([\"mean\", \"std\"])\n",
    "            .values\n",
    "        )\n",
    "        new_df.loc[curr_period] = (\n",
    "            new_df.loc[curr_period].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "        )\n",
    "    solar = pd.concat([solar, new_df], axis=1)\n",
    "    solar[train_cols] = solar[train_cols].astype(float)\n",
    "\n",
    "    # sample at output frequency\n",
    "    solar = solar.loc[solar[\"timedelta\"].dt.seconds % (win * 60) == 0].reset_index()\n",
    "\n",
    "    return solar, train_cols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model architecture\n",
    "\n",
    "The model consists of a set of convolutional layers which detect patterns at progressively longer time spans. At each convolutional \n",
    "layer (except the last), a convolutional filter is applied having size 6 with stride 3, which reduces the size of the output data \n",
    "relative to the input. (The last convolution has small input size, so it just convolves all its 9 inputs \n",
    "together.) The earlier layers recognize low-level features on short time-spans, and these are outputs aggregated into higher-level\n",
    "patterns spanning longer time ranges in the later layers. Cropping is applied at each layer which removes a few data points at the beginning at the \n",
    "sequence to ensure the result will be exactly divisible by 6, so that the last application of the convultional filter will \n",
    "capture the data at the very end of the sequence. Following all the convolutional \n",
    "layers is a layer which concatenates the last data point of each of the convolution outputs. This concatenation is then fed into a dense layer. The idea of taking the last data point of each convolution \n",
    "is that it represents the patterns  at different timespans leading up to the prediction time: for example, the last data point \n",
    "of the first layer gives the features of the hour before the prediction time, then the second layer gives \n",
    "the last 6 hours, etc.\n",
    "\n",
    "The architecture is somewhat similar to a widely used architecture for image segmentation, the U-Net introduced by Ronneberger, Fischer, and Brox (https://arxiv.org/abs/1505.04597). The U-Net consists of a \"contracting path\", a series of convolutional layers which condense the image, followed by an \"expansive path\" of up-convolution layers which expand the outputs back to the scale of the original image. Combining small-scale and large-scale features allows the network to make localised predictions that also take account of larger surrounding patterns.\n",
    "\n",
    "The idea is also similar to the Temporal Convolutional Network described by Bai, Kolter, and Koltun (https://arxiv.org/abs/1803.01271); however their architecture uses residual (i.e. additive) connections to blend the low-level and high-level features, rather than concatenations.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define the structure of the neural network for 1-minute data\n",
    "\n",
    "def define_model_cnn_1_min() -> Tuple[\n",
    "    tf.keras.Model, List[np.ndarray], int, float, int\n",
    "]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        model: keras model\n",
    "        initial_weights: Array of initial weights used to reset the model to its\n",
    "            original state\n",
    "        epochs: Number of epochs\n",
    "        lr: Learning rate\n",
    "        bs: Batch size\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tf.keras.layers.Input((6 * 24 * 7, 13))\n",
    "    conv1 = tf.keras.layers.Conv1D(50, kernel_size=6, strides=3, activation=\"relu\")(\n",
    "        inputs\n",
    "    )\n",
    "    trim1 = tf.keras.layers.Cropping1D((5, 0))(\n",
    "        conv1\n",
    "    )  # crop from left so resulting shape is divisible by 6\n",
    "    conv2 = tf.keras.layers.Conv1D(50, kernel_size=6, strides=3, activation=\"relu\")(\n",
    "        trim1\n",
    "    )\n",
    "    trim2 = tf.keras.layers.Cropping1D((1, 0))(conv2)\n",
    "    conv3 = tf.keras.layers.Conv1D(30, kernel_size=6, strides=3, activation=\"relu\")(\n",
    "        trim2\n",
    "    )\n",
    "    trim3 = tf.keras.layers.Cropping1D((5, 0))(conv3)\n",
    "    conv4 = tf.keras.layers.Conv1D(30, kernel_size=6, strides=3, activation=\"relu\")(\n",
    "        trim3\n",
    "    )\n",
    "    conv5 = tf.keras.layers.Conv1D(30, kernel_size=9, strides=9, activation=\"relu\")(\n",
    "        conv4\n",
    "    )\n",
    "    # extract last data point of previous convolutional layers (left-crop all but one)\n",
    "    comb1 = tf.keras.layers.Concatenate(axis=2)(\n",
    "        [\n",
    "            conv5,\n",
    "            tf.keras.layers.Cropping1D((334, 0))(conv1),\n",
    "            tf.keras.layers.Cropping1D((108, 0))(conv2),\n",
    "            tf.keras.layers.Cropping1D((34, 0))(conv3),\n",
    "            tf.keras.layers.Cropping1D((8, 0))(conv4),\n",
    "        ]\n",
    "    )\n",
    "    dense = tf.keras.layers.Dense(50, activation=\"relu\")(comb1)\n",
    "    output = tf.keras.layers.Flatten()(tf.keras.layers.Dense(1)(dense))\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "    initial_weights = model.get_weights()\n",
    "    epochs = 3\n",
    "    lr = 0.00025\n",
    "    bs = 32\n",
    "    return model, initial_weights, epochs, lr, bs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Ensembling\n",
    "\n",
    "The final model is an ensemble of 5 models with the same structure, trained on different subsets of the data (these are often called \"cross-validation folds\"). This is a common technique in machine learning. The idea is that each model only imperfectly captures the \"true\" relationship betweeen the input and output variables, and partly fits to noise in the training data. But if we average several models, the random noise components will approximately cancel each other out, leaving a more accurate prediction of the true relationship.\n",
    "\n",
    "The training data is segmented by month, and for each model 20\\% of months are excluded. The data is split by months \n",
    "rather than hours because successive hours are likely to be correlated, meaning test and train sets would be \n",
    "more similar, reducing the benefit of the ensemble. Separate models are trained for times $t$ and $t+1$, yielding 10 models in total. To ensure each fold contains a mixture of extreme and moderate <i>Dst</i> periods, we order the months by average <i>Dst</i>, and then leave out every fifth month for each fold (e.g. in fold 0 we exclude months 4, 9, 14..., then in fold 1 we exclude months 1, 10, 15, ...).\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">For reproducibility, we set the numpy, keras, and python random seeds. Results may still not reproduce exactly due to hardware differences. </div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train ensembling model on different subsets of data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define a function to automatically train an ensemble of models on different\n",
    "# subsets of the data\n",
    "\n",
    "def train_on_prepared_data(\n",
    "    prepared_data: pd.DataFrame,\n",
    "    model: tf.keras.Model,\n",
    "    initial_weights: Union[List[List[np.ndarray]], List[np.ndarray]],\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    bs: int,\n",
    "    train_cols,\n",
    "    num_models: int = 1,\n",
    "    output_folder: str = \"trained_models\",\n",
    "    data_frequency: str = \"minute\",\n",
    "    early_stopping: bool = False,\n",
    ") -> Optional[List[float]]:\n",
    "    \"\"\" \n",
    "    Train and save ensemble of models\n",
    "    \n",
    "    Args:\n",
    "        prepared_data: DataFrame containing solar wind data, sunspots data, and labels\n",
    "        model: keras neural network model,\n",
    "        initial_weights: initial weights for the model. Either a single set of weights\n",
    "            to use for all models, or a list of 2 * num_models sets of weights,\n",
    "            representing weights for the t models followed by the t + 1 models.\n",
    "        epochs: number of training epochs,\n",
    "        lr: learning rate,\n",
    "        bs: batch size,\n",
    "        train_cols: columns of ``prepared_data`` to use for training. Must match the\n",
    "            input size defined in ``model_definer``.\n",
    "        num_models: Number of models to train.\n",
    "        output_folder: Path to the directory where models will be saved\n",
    "        data_frequency: frequency of the training data: \"minute\" or \"hour\"\n",
    "        early_stopping: If ``True``, stop model training when validation loss stops\n",
    "            decreasing.\n",
    "    Returns:\n",
    "        out-of-sample accuracy: If ``num_models > 1``, returns list of length\n",
    "            ``num_models`` containing RMSE values for out-of-sample predictions for each\n",
    "            model.\n",
    "    \"\"\"\n",
    "\n",
    "    # define model and training parameters\n",
    "    if data_frequency == \"minute\":\n",
    "        sequence_length = 6 * 24 * 7\n",
    "    else:\n",
    "        sequence_length = 24 * 7\n",
    "\n",
    "    oos_accuracy = []\n",
    "    # train on sequences ending at the start of an hour\n",
    "    valid_bool = prepared_data[\"timedelta\"].dt.seconds % 3600 == 0\n",
    "    # exclude periods where data contains nans\n",
    "    nans_in_train = (\n",
    "        prepared_data[train_cols]\n",
    "        .isnull()\n",
    "        .any(axis=1)\n",
    "        .rolling(window=sequence_length + 1, center=False)\n",
    "        .max()\n",
    "    )\n",
    "    valid_bool = valid_bool & (nans_in_train == 0)\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "    prepared_data[\"month\"] = prepared_data[\"month\"].astype(int)\n",
    "    mean_dst_by_month = prepared_data.groupby(\"month\")[\"target\"].mean()\n",
    "    mean_dst_by_month.sort_values(inplace=True)\n",
    "    months = mean_dst_by_month.index\n",
    "    # remove the first week from each period, because not enough data for prediction\n",
    "    valid_ind_arr = []\n",
    "    for p in prepared_data[\"period\"].unique():\n",
    "        all_p = prepared_data.loc[\n",
    "            (prepared_data[\"period\"] == p) & valid_bool\n",
    "        ].index.values[24 * 7 :]\n",
    "        valid_ind_arr.append(all_p)\n",
    "    valid_ind = np.concatenate(valid_ind_arr)\n",
    "    non_exclude_ind = prepared_data.loc[\n",
    "        ~prepared_data[\"train_exclude\"].astype(bool)\n",
    "    ].index.values\n",
    "    #np.random.shuffle(months)\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "    )\n",
    "    callbacks = []\n",
    "    if early_stopping:\n",
    "        callbacks.append(es_callback)\n",
    "        epochs = 100\n",
    "    for model_ind in range(num_models):\n",
    "        if isinstance(initial_weights[0], np.ndarray):\n",
    "            initial_weights_t = initial_weights\n",
    "            initial_weights_t_plus_1 = initial_weights\n",
    "        else:\n",
    "            initial_weights_t = initial_weights[model_ind]\n",
    "            initial_weights_t_plus_1 = initial_weights[num_models + model_ind]\n",
    "        # t model\n",
    "        tf.keras.backend.clear_session()\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "        )\n",
    "        model.set_weights(initial_weights_t)\n",
    "        if (num_models > 1) or early_stopping:\n",
    "            # define train and test sets\n",
    "            month_freq = num_models if num_models > 1 else 5\n",
    "            # leave out every month_freq month; since months are ordered, these are approximately\n",
    "            # evenly distributed among the range of dst values\n",
    "            leave_out_months = months[model_ind :: month_freq]\n",
    "            leave_out_months_ind = prepared_data.loc[\n",
    "                valid_bool & prepared_data[\"month\"].isin(leave_out_months)\n",
    "            ].index.values\n",
    "            curr_months_ind = prepared_data.loc[\n",
    "                valid_bool & (~prepared_data[\"month\"].isin(leave_out_months))\n",
    "            ].index.values\n",
    "            train_ind = np.intersect1d(\n",
    "                np.intersect1d(valid_ind, curr_months_ind), non_exclude_ind\n",
    "            )\n",
    "            test_ind = np.intersect1d(valid_ind, leave_out_months_ind)\n",
    "            train_gen = DataGen(\n",
    "                prepared_data[train_cols].values,\n",
    "                train_ind,\n",
    "                prepared_data[\"target\"].values.flatten(),\n",
    "                bs,\n",
    "                sequence_length,\n",
    "            )\n",
    "            test_gen = DataGen(\n",
    "                prepared_data[train_cols].values,\n",
    "                test_ind,\n",
    "                prepared_data[\"target\"].values.flatten(),\n",
    "                bs,\n",
    "                sequence_length,\n",
    "            )\n",
    "            model.fit(\n",
    "                train_gen,\n",
    "                validation_data=test_gen,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "            oos_accuracy.append(model.evaluate(test_gen, verbose=2)[1])\n",
    "            print(\"Out of sample accuracy: \", oos_accuracy)\n",
    "            print(\"Out of sample accuracy mean: {}\".format(np.mean(oos_accuracy)))\n",
    "        else:\n",
    "            # fit on all data\n",
    "            train_ind = valid_ind\n",
    "            train_gen = DataGen(\n",
    "                prepared_data[train_cols].values,\n",
    "                train_ind,\n",
    "                prepared_data[\"target\"].values.flatten(),\n",
    "                bs,\n",
    "                sequence_length,\n",
    "            )\n",
    "            model.fit(train_gen, epochs=epochs, verbose=1, callbacks=callbacks)\n",
    "        model.save(os.path.join(output_folder, \"model_t_{}.h5\".format(model_ind)))\n",
    "        if early_stopping:\n",
    "            with open(os.path.join(output_folder, \"log.txt\"), \"a\") as f:\n",
    "                es_iter = es_callback.stopped_epoch - es_callback.patience + 1\n",
    "                f.write(f\"\\n\\nEarly stopping iterations: {es_iter}\")\n",
    "        # t + 1 model\n",
    "        tf.keras.backend.clear_session()\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "        )\n",
    "        model.set_weights(initial_weights_t_plus_1)\n",
    "        data_gen = DataGen(\n",
    "            prepared_data[train_cols].values,\n",
    "            train_ind,\n",
    "            prepared_data[\"target_shift\"].values.flatten(),\n",
    "            bs,\n",
    "            sequence_length,\n",
    "        )\n",
    "        if early_stopping and (es_callback.stopped_epoch > 0):\n",
    "            model.fit(\n",
    "                data_gen,\n",
    "                epochs=es_callback.stopped_epoch - es_callback.patience + 1,\n",
    "                verbose=1,\n",
    "            )\n",
    "        else:\n",
    "            model.fit(data_gen, epochs=epochs, verbose=1)\n",
    "        model.save(\n",
    "            os.path.join(output_folder, \"model_t_plus_one_{}.h5\".format(model_ind))\n",
    "        )\n",
    "    if num_models > 1:\n",
    "        return oos_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data generator\n",
    "\n",
    "We make a data generator to generate batches of input data and labels to feed to the keras model. Generating the data dynamically this way saves memory by avoiding the need to assemble all the data in a large array before training. (This large array would consume much more memory than the original data, because there is a lot of overlap in input data between subsequent prediction times, and thus a lot of repetition of the input data). Data is shuffled at the end of each training epoch, so each epoch's batches are different.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">Data generator to dynamically generate batches of training data for keras model. Each batch consists of multiple time series sequences. See the keras documentation for more details:\n",
    "https://keras.io/getting_started/faq/#what-do-sample-batch-and-epoch-mean\n",
    "</div>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define data generator\n",
    "\n",
    "class DataGen(tf.keras.utils.Sequence):\n",
    "   \n",
    "    def __init__(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        valid_inds: np.ndarray,\n",
    "        y: Optional[np.ndarray] = None,\n",
    "        batch_size: int = 32,\n",
    "        length: int = 24 * 6 * 7,\n",
    "        shuffle: bool = True,\n",
    "    ):\n",
    "        \"\"\"Construct the data generator.\n",
    "\n",
    "        If ``y`` is not ``None``, will generate batches of pairs of x and y data,\n",
    "        suitable for training. If ``y`` is ``None``, will generate batches of ``x``\n",
    "        data only, suitable for prediction.\n",
    "\n",
    "        ``x`` and ``y`` data must already be ordered by period and time. The training\n",
    "        sample generated for an index ``i`` in ``valid_ind`` will have target ``y[i]``\n",
    "        and ``x`` variables from rows ``(i - length + 1)`` to ``i`` (inclusive) of\n",
    "        ``x``.  If there are multiple periods, there should be at least ``(length - 1)``\n",
    "        data points before the first ``valid_ind`` in each period, otherwise the\n",
    "        sequence for that valid_ind will include data from a previous period.\n",
    "\n",
    "        Args:\n",
    "            x: Array containing the x variables ordered by period and time\n",
    "            y: ``None`` or array containing the targets corresponding to the ``x``\n",
    "                variables\n",
    "            batch_size: Size of training batches\n",
    "            valid_inds: Array of ``int`` containing the indices which are valid\n",
    "                end-points of training sequences (for example, we may set ``valid_inds``\n",
    "                so it contains only the data points at the start of each hour).\n",
    "            length: Number of data points in each sequence of the batch\n",
    "            shuffle: Whether to shuffle ``valid_ind`` before training and after\n",
    "                each epoch. For training, it is recommended to set this to ``True``, so\n",
    "                each batch contains a varied sample of data from different times. For\n",
    "                prediction, it should be set to ``False``, so that the predicted values\n",
    "                are in the same order as the input data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.length = length\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_inds = np.copy(valid_inds)\n",
    "        self.shuffle = shuffle\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.valid_inds)\n",
    "\n",
    "    def __get_y__(self):\n",
    "        \"\"\"Return the array of labels indexed by ``valid_ind``.\"\"\"\n",
    "        if self.y is None:\n",
    "            raise RuntimeError(\"Generator has no y data.\")\n",
    "        else:\n",
    "            return self.y[self.valid_inds]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches in each epoch.\"\"\"\n",
    "        return int(np.ceil(len(self.valid_inds) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generate a batch. ``idx`` is the index of the batch in the training epoch.\"\"\"\n",
    "        if (idx < self.__len__() - 1) or (len(self.valid_inds) % self.batch_size == 0):\n",
    "            num_samples = self.batch_size\n",
    "        else:\n",
    "            num_samples = len(self.valid_inds) % self.batch_size\n",
    "        x = np.empty((num_samples, self.length, self.x.shape[1]))\n",
    "        end_indexes = self.valid_inds[\n",
    "            idx * self.batch_size : (idx + 1) * self.batch_size\n",
    "        ]\n",
    "        for n, i in enumerate(end_indexes):\n",
    "            x[n] = self.x[i - self.length : i, :]\n",
    "        if self.y is None:\n",
    "            return x\n",
    "        else:\n",
    "            y = self.y[end_indexes]\n",
    "            return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Code to run at the end of each training epoch.\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.valid_inds)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model\n",
    "\n",
    "Here we execute the data preparation and training functions we defined earlier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preprocess input data\n",
    "output_folder = \"trained_models_cnn\"\n",
    "solar_1_min, train_cols = prepare_data_1_min(\n",
    "    solar_train, sunspots_train, dst_train, output_folder=output_folder, norm_df=None\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train model\n",
    "model_def_1_min = define_model_cnn_1_min\n",
    "model, initial_weights, epochs, lr, bs = model_def_1_min()\n",
    "train_on_prepared_data(\n",
    "        solar_1_min,\n",
    "        model,\n",
    "        initial_weights,\n",
    "        epochs,\n",
    "        lr,\n",
    "        bs,\n",
    "        train_cols,\n",
    "        5,\n",
    "        output_folder,\n",
    "        \"minute\",\n",
    "        False\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define function to load saved models\n",
    "\n",
    "def load_models(\n",
    "    input_folder: str, num_models: int,\n",
    ") -> Tuple[List[tf.keras.Model], List[tf.keras.Model], pd.DataFrame]:\n",
    "    \"\"\"Define the model structure and load the saved weights of the trained models.\n",
    "\n",
    "    Args:\n",
    "        input_folder: Path to location where model weights are saved\n",
    "        num_models: Number of models trained for each of ``t`` and ``t + 1`` (total\n",
    "            number of models in folder should be ``2 * num_models``)\n",
    "\n",
    "    Returns:\n",
    "        model_t_arr: List of models for time ``t``\n",
    "        model_t_plus_one_arr: List of models for time ``t + 1``\n",
    "        norm_df: DataFrame of scaling factors to normalize the data\n",
    "    \"\"\"\n",
    "    model_t_arr = []\n",
    "    model_t_plus_one_arr = []\n",
    "    for i in range(num_models):\n",
    "        model = tf.keras.models.load_model(\n",
    "            os.path.join(input_folder, \"model_t_{}.h5\".format(i))\n",
    "        )\n",
    "        model_t_arr.append(model)\n",
    "        model = tf.keras.models.load_model(\n",
    "            os.path.join(input_folder, \"model_t_plus_one_{}.h5\".format(i))\n",
    "        )\n",
    "        model_t_plus_one_arr.append(model)\n",
    "    norm_df = pd.read_csv(os.path.join(input_folder, \"norm_df.csv\"), index_col=0)\n",
    "    return model_t_arr, model_t_plus_one_arr, norm_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_folder = \"trained_models_cnn\"\n",
    "model_t_arr, model_t_plus_1_arr, norm_df = load_models(output_folder, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Measure overall performance\n",
    "\n",
    "We score the model on the private dataset, using the root mean squared error metric used in the competition. In the competition, the prediction function was called repeatedly on one week of input data at at time. Here we can speed up the prediction by using the data generator we defined above.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Make predictions for multiple times; faster than ``predict_one_time``.\n",
    "Input data must be sorted by period and timT and have 1-minute frequency, and there must be at least 1 week of data before the first prediction time. </div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Calculate the RMSE for ensembling model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define prediction function\n",
    "\n",
    "def predict_batch(\n",
    "    solar: pd.DataFrame,\n",
    "    sunspots: pd.DataFrame,\n",
    "    prediction_times: pd.DataFrame,\n",
    "    model_t_arr: List[tf.keras.Model],\n",
    "    model_t_plus_one_arr: List[tf.keras.Model],\n",
    "    norm_df: pd.DataFrame,\n",
    "    frequency: str,\n",
    "    output_folder: str,\n",
    "    comb_model: bool = False\n",
    ") -> Tuple[pd.DataFrame, List, List]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        solar: DataFrame containing solar wind data\n",
    "        sunspots: DataFrame containing sunspots data\n",
    "        prediction_times: DataFrame with a single column `timedelta` for which to make\n",
    "            predictions. For each value ``t``, return predictions for ``t`` and\n",
    "            ``t`` plus one hour.\n",
    "        model_t_arr: List of models for time ``t``\n",
    "        model_t_plus_one_arr: List of models for time ``(t + 1)``\n",
    "        norm_df: Scaling factors to normalize the data\n",
    "        frequency: frequency of the model, \"minute\", \"hour\" or \"hybrid\"\n",
    "        output_folder: Path to the directory where normalisation dataframe will be saved\n",
    "        comb_model: if ``True`` assumes a single model that predicts ``t`` and ``t+1``\n",
    "\n",
    "    Returns:\n",
    "        predictions: DataFrame with columns ``timedelta``, ``period``, ``prediction_t``\n",
    "            and ``prediction_t_plus_1``\n",
    "        t0_predictions_set: Dataframe containing the predicted t0 results from five individual models \n",
    "        t1_predictions_set: Dataframe containing the predicted t1 results from five individual models \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # validate input data\n",
    "    solar[\"timedelta\"] = pd.to_timedelta(solar[\"timedelta\"])\n",
    "    diff = solar[\"timedelta\"].diff()\n",
    "    diff.loc[solar[\"period\"] != solar[\"period\"].shift()] = np.nan\n",
    "    valid_diff = solar[\"period\"] == solar[\"period\"].shift(1)\n",
    "    if (frequency in [\"minute\", \"hybrid\"]) and np.any(\n",
    "        diff.loc[valid_diff] != dt.timedelta(minutes=1)\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Input data must be sorted by period and time and have 1-minute frequency.\"\n",
    "        )\n",
    "    elif (frequency == \"hour\") and np.any(\n",
    "        diff.loc[valid_diff] != dt.timedelta(hours=1)\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Input data must be sorted by period and time and have 1-hour frequency.\"\n",
    "        )\n",
    "\n",
    "    # add column to solar to indicate which times we must predict\n",
    "    prediction_times[\"prediction_time\"] = True\n",
    "    solar = pd.merge(solar, prediction_times, on=[\"period\", \"timedelta\"], how=\"left\")\n",
    "    solar[\"prediction_time\"] = solar[\"prediction_time\"].fillna(False)\n",
    "    solar.sort_values([\"period\", \"timedelta\"], inplace=True)\n",
    "    solar.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # prepare data\n",
    "    if frequency == \"minute\":\n",
    "        solar, train_cols = prepare_data_1_min(\n",
    "            solar.copy(), sunspots.copy(), output_folder=output_folder, norm_df=norm_df\n",
    "        )\n",
    "    elif frequency == \"hour\":\n",
    "        solar, train_cols = prepare_data_hourly(\n",
    "            solar.copy(), sunspots.copy(), norm_df=norm_df\n",
    "        )\n",
    "    elif frequency == \"hybrid\":\n",
    "        solar, train_cols = prepare_data_hybrid(\n",
    "            solar.copy(), None, sunspots.copy(), norm_df=norm_df, freq_for_1_min_data=\"10_minute\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid frequency {frequency}\")\n",
    "\n",
    "    # check there is 1 week of data before each valid time\n",
    "    min_data_by_period = solar.groupby(\"period\")[\"timedelta\"].min().to_frame(\"min_time\")\n",
    "    min_data_by_period[\"min_prediction_time\"] = (\n",
    "        solar.loc[solar[\"prediction_time\"]].groupby(\"period\")[\"timedelta\"].min()\n",
    "    )\n",
    "    min_data_by_period[\"data_before_first_prediction\"] = (\n",
    "        min_data_by_period[\"min_prediction_time\"] - min_data_by_period[\"min_time\"]\n",
    "    )\n",
    "    if min_data_by_period[\"data_before_first_prediction\"].min() < dt.timedelta(days=7):\n",
    "        raise RuntimeError(\n",
    "            \"There must be at least 1 week of data before the first prediction time in each period.\"\n",
    "        )\n",
    "\n",
    "    # valid_ind will be the endpoints of the sequences generated by the data generator;\n",
    "    # these must be 1 minute/hour before the prediction time\n",
    "    solar[\"valid_ind\"] = solar[\"prediction_time\"].fillna(False)\n",
    "\n",
    "    # make prediction\n",
    "    predictions = pd.DataFrame(prediction_times[[\"timedelta\", \"period\"]].copy())\n",
    "    valid_ind = solar.loc[solar[\"valid_ind\"]].index.values\n",
    "    sequence_length = 24 * 6 * 7 if frequency in [\"minute\", \"hybrid\"] else 24 * 7\n",
    "    datagen = DataGen(\n",
    "        solar[train_cols].values,\n",
    "        valid_ind,\n",
    "        y=None,\n",
    "        batch_size=100,\n",
    "        length=sequence_length,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    #Create the array for analyzing five models later.\n",
    "    t0_predictions_set = []\n",
    "    t1_predictions_set = []\n",
    "\n",
    "    predictions[\"prediction_t\"] = 0\n",
    "    predictions[\"prediction_t_plus_1\"] = 0\n",
    "    if comb_model:\n",
    "        for m in model_t_arr:\n",
    "            predictions[[\"prediction_t\", \"prediction_t_plus_1\"]] = np.array(m.predict(datagen))\n",
    "    else:\n",
    "        for m in model_t_arr:\n",
    "            curr_model = np.array(m.predict(datagen)).flatten()\n",
    "            t0_predictions_set.append(curr_model)\n",
    "            predictions[\"prediction_t\"] += curr_model\n",
    "        predictions[\"prediction_t\"] /= len(model_t_arr)\n",
    "\n",
    "        for m in model_t_plus_one_arr:\n",
    "            curr_model = np.array(m.predict(datagen)).flatten()\n",
    "            t0_predictions_set.append(curr_model)\n",
    "            predictions[\"prediction_t_plus_1\"] += curr_model\n",
    "        predictions[\"prediction_t_plus_1\"] /= len(model_t_plus_one_arr)\n",
    "\n",
    "    # restrict to allowed range\n",
    "    predictions[\"prediction_t\"] = np.maximum(\n",
    "        -2000, np.minimum(500, predictions[\"prediction_t\"])\n",
    "    )\n",
    "    predictions[\"prediction_t_plus_1\"] = np.maximum(\n",
    "        -2000, np.minimum(500, predictions[\"prediction_t_plus_1\"])\n",
    "    )\n",
    "\n",
    "    return predictions, t0_predictions_set, t1_predictions_set\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calibrate time column"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dst_test[\"timedelta\"] = pd.to_timedelta(dst_test[\"timedelta\"])\n",
    "# exclude times in the first week + 1 hour of dst_test\n",
    "dst_test = dst_test.loc[dst_test[\"timedelta\"] >= dt.timedelta(days=7, hours=1)].copy()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Measure RMSE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# measure RMSE\n",
    "predictions, t0_predictions_set, t1_predictions_set = predict_batch(\n",
    "    solar_test.copy(), sunspots_test, dst_test, model_t_arr, model_t_plus_1_arr, norm_df, \"minute\", output_folder\n",
    ")\n",
    "\n",
    "#merge true data with prediction results\n",
    "dst_test_1_min = pd.merge(dst_test, predictions, \"left\", [\"timedelta\", \"period\"])\n",
    "dst_test_1_min[\"dst_t_plus_1\"] = dst_test_1_min.groupby(\"period\")[\"dst\"].shift(-1)\n",
    "\n",
    "aveModel_rmse_t = np.sqrt(\n",
    "    mean_squared_error(dst_test_1_min[\"dst\"].values, dst_test_1_min[\"prediction_t\"].values)\n",
    ")\n",
    "\n",
    "valid_ind = dst_test_1_min[\"dst_t_plus_1\"].notnull()\n",
    "aveModel_rmse_t_plus_1 = np.sqrt(\n",
    "    mean_squared_error(\n",
    "        dst_test_1_min.loc[valid_ind, \"dst_t_plus_1\"].values,\n",
    "        dst_test_1_min.loc[valid_ind, \"prediction_t_plus_1\"].values,\n",
    "    )\n",
    ")\n",
    "print(f\"RMSE for time t: {aveModel_rmse_t:0.2f}\")\n",
    "print(f\"RMSE for time t+1: {aveModel_rmse_t_plus_1:0.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyze model uncertainty\n",
    "\n",
    "The final model is an ensemble of 5 models with the same structure, trained on different cross-validation folds. To understand the uncertainty of 5 models, we use errorbar to indicate the spreads of 5 models. The limits is the maximum and minimum difference of predicted <i>Dst</i> with final model's <i>Dst</i> in each time steps.\n",
    "\n",
    "In our case, the size of ensamble is various in each time steps. The larger the peak intends to have the larger spreads."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define the function for selecting the max and min difference based in each time steps for each five models\n",
    "def create_model_uncertainty_array(N: int, predictions_set: List):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      N: number of time steps\n",
    "      predictions_set: predicted Dst from five trained models\n",
    "\n",
    "    Returns:\n",
    "      uncertainty: arrary for holding the difference of maximum and minmium predicted Dst among five models\n",
    "    \"\"\"\n",
    "\n",
    "    uncertainty = []\n",
    "    M = len(predictions_set)\n",
    "    time_steps = [0] * M\n",
    "\n",
    "    for j in range(N):\n",
    "        for i in range(M):\n",
    "            time_steps[i] = predictions_set[i][j]\n",
    "        max_val = max(time_steps)\n",
    "        min_val = min(time_steps)\n",
    "        uncertainty.append(max_val - min_val)\n",
    "\n",
    "    return uncertainty\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define the function to plot Errorbar for each model\n",
    "def plot_ErrorBar(start: int, end: int, true: np.ndarray, pred: np.ndarray, predErr: List, timedelta_arr: List):\n",
    "\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    time = np.arange(0, len(true))\n",
    "\n",
    "    plt.errorbar(time[start:end], pred[start:end], predErr[start:end], color=\"b\", label=\"limits of predicted Dst\", elinewidth = 2)\n",
    "    plt.plot(time[start:end], true[start:end], color=\"r\", label=\"true data\")\n",
    "\n",
    "    plt.xticks(time[start:end], timedelta_arr[start:end], rotation=45)\n",
    "    plt.locator_params(nbins=8)\n",
    "    plt.title(\"Uncertainty Quantifucation(UQ) of the Ensamble CNN Model\")\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.xlabel(\"hours\")\n",
    "    plt.ylabel(\"Dst\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visulize the Error Bar\n",
    "The error bar indicates the maximum and minimum Dst value predicted by five models. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> We only demonstrate the UQ plot focus on the peak. User can adjust the index range of Dst and predicted Dst from plot_ErrorBar(). \n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the limits for the prediction model\n",
    "from matplotlib import dates\n",
    "\n",
    "N = len(predictions)\n",
    "t0_uncern = create_model_uncertainty_array(N, t0_predictions_set)\n",
    "time_arr = dst_test_1_min[\"timedelta\"].astype(str).tolist()\n",
    "\n",
    "# Set the start and end hours you plan to plot\n",
    "start = 1600\n",
    "end = 1800\n",
    "plot_ErrorBar(start, end, dst_test_1_min[\"dst\"].values, dst_test_1_min[\"prediction_t\"].values, t0_uncern, time_arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explainable AI(XAI)\n",
    "\n",
    "The idea is to evaluate the importance of feature to the model. Based on Christoph Molnar's \"Interpretable Machine Learning\" section and Fisher, Rudin, and Dominici (2018), if the feature is important, after permuting the feature e.g. split, swap or shuffling, the model error will increase. On the vice-versa, if the feature is unimportant, it leaves the model error almost unchanged.  \n",
    "\n",
    "\n",
    "Resources:\n",
    "*   Christoph Molnar's \"Interpretable Machine Learning\" section on [Permutation Feature Importance](https://christophm.github.io/interpretable-ml-book/feature-importance.html), and see also their <b>argument for using Test data</b> for Permutation Importance evaluation, which we have chosen to do here.\n",
    "*   [See this illustrative graphic demonstrating single- and multi-pass Permutation Importance](https://permutationimportance.readthedocs.io/en/latest/methods.html#permutation-importance)\n",
    "*   [Permutation Feature Importance in the <i>scikit-learn</i> module](https://scikit-learn.org/stable/modules/permutation_importance.html)\n",
    "\n",
    "Basically, we can split and swap the feature datasets one feature at a time and compare the resultant RMSE. We take a programming convenience shortcut and <b>simply reverse each feature</b> vector rather than split and swap and we expect the same results. We'll do this, i.e. permute each feature vector, one at a time. Then, calcualte the ratio of RMSE with the RMSE without permuting features which we computed at previous section. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def permutation_importance(solar: pd.DataFrame,\n",
    "    sunspots: pd.DataFrame,\n",
    "    dst: pd.DataFrame,\n",
    "    model_t_arr: List[tf.keras.Model],\n",
    "    model_t_plus_one_arr: List[tf.keras.Model],\n",
    "    norm_df: pd.DataFrame,\n",
    "    permute_cols: List,\n",
    "    output_folder: str) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      solar: Dataframe containing solar wind data.\n",
    "      sunspots: Dataframe containing sunspots data.\n",
    "      dst: Dataframe containing Dst label data\n",
    "      model_t_arr: List of models for time ``t``\n",
    "      model_t_plus_one_arr: List of models for time ``t + 1``\n",
    "      norm_df: DataFrame of scaling factors to normalize the data\n",
    "      permute_cols: List of features intend to be permeuted\n",
    "      output_folder: the folder location for trained models\n",
    "    \n",
    "    Returns:\n",
    "      rmse_permute_df: Dataframe containing RMSE corresponding to permuted feature\n",
    "    \"\"\"\n",
    "\n",
    "    rmse_permute_df = pd.DataFrame(np.zeros((1, len(permute_cols))), columns=permute_cols)\n",
    "\n",
    "    for feature in permute_cols:\n",
    "\n",
    "        test_for_permute = solar.copy(deep=True)\n",
    "        # Approximate split permutation by simply reversing the data in this feature\n",
    "        test_for_permute[feature].values[:] = test_for_permute[feature].values[::-1]\n",
    "\n",
    "        print(f\"feature: {feature}\")\n",
    "        # get prediction\n",
    "        predictions, t0_predictions_set, t1_predictions_set = predict_batch(\n",
    "            test_for_permute, sunspots, dst, model_t_arr, model_t_plus_one_arr, norm_df, \"minute\", output_folder\n",
    "        )\n",
    "\n",
    "\n",
    "        dst_test_1_min = pd.merge(dst, predictions, \"left\", [\"timedelta\", \"period\"])\n",
    "\n",
    "        loss_t = np.sqrt(\n",
    "            mean_squared_error(dst_test_1_min[\"dst\"].values, dst_test_1_min[\"prediction_t\"].values)\n",
    "        )\n",
    "\n",
    "        rmse_permute_df[feature] = loss_t\n",
    "\n",
    "        print('%s: %f rmse nano-Tesla' % (feature, rmse_permute_df[feature]))\n",
    "\n",
    "        # delete arrays in every iterations to save RAM usag for colab\n",
    "        del predictions\n",
    "        del t0_predictions_set\n",
    "        del t1_predictions_set\n",
    "\n",
    "    return rmse_permute_df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Delete the predictions results to avoid running out of RAM\n",
    "del predictions\n",
    "del t0_predictions_set\n",
    "del t1_predictions_set\n",
    "\n",
    "# define the features plans to permute\n",
    "permute_cols = [\"bx_gsm\", \"by_gsm\", \"bz_gsm\", \"bt\", \"speed\", \"density\", \"temperature\"] \n",
    "\n",
    "rmse_permute_df = permutation_importance(solar_test.copy(), sunspots_test, dst_test, \n",
    "                                         model_t_arr, model_t_plus_1_arr, norm_df, permute_cols, output_folder)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The score of feature importance from high to low"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ratio the Permuted RMSE to the overall RMSE and sort in order of importance\n",
    "print('In order of most important feature first to least important by rmse(j)/rmse:')\n",
    "rmse_ratio_df = (rmse_permute_df/aveModel_rmse_t).sort_values(ascending=False, by=0, axis=1)\n",
    "rmse_ratio_df.T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualize the Permutation Importance outcome"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(rmse_ratio_df.columns, rmse_ratio_df.values.T, 'x-')\n",
    "plt.xticks(rotation=270)\n",
    "plt.ylabel('RMSE Ratio %')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Event Case Study: Measure performance in different <i>Dst</i> ranges\n",
    "\n",
    "Although the RMSE gives an indication of model performance, for pratical applications we are mainly interested in accurate predictions during storm events, i.e. when the <i>Dst</i> has a large negative value. Below we plot the performance binned by DST ranges.\n",
    "\n",
    "Unfortunately, it is difficult to accurately estimate performance at extreme times because storm events are very rare. We also show below the number of hourly values in each range; note that since storm events typically last 1-2 days, most of these hours occur very close to each other, and thus do not really represent distinct events."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot errors in different <i>Dst</i> ranges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define plotting function\n",
    "def plot_binned_RMSE(actual: np.ndarray, predicted: np.ndarray, bin_edges):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        actual: true values of dst\n",
    "        predicted: predicted values of dst\n",
    "        bin_edges: edges of the bins by which to group the data\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame({\"actual\": actual, \"predicted\": predicted})\n",
    "    df[\"bin\"] = pd.cut(df[\"actual\"], bin_edges)\n",
    "    df[\"sq_err\"] = (df[\"actual\"] - df[\"predicted\"]) ** 2\n",
    "    RMSE_by_bin = np.sqrt(df.groupby(\"bin\")[\"sq_err\"].mean())\n",
    "    plt.plot(RMSE_by_bin.values, marker='.', markersize=15)\n",
    "    labels = [s.replace(\",\", \",\\n\") for s in RMSE_by_bin.index.astype(str).values]\n",
    "    plt.xticks(ticks=np.arange(len(RMSE_by_bin)), labels=labels)\n",
    "    plt.xlabel(\"Dst range\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "\n",
    "# Produce RMSE plot, binned by Dst range.\n",
    "plt.figure(figsize=(15, 12))\n",
    "bins = [-np.inf, -300, -200, -100, -50, 0, np.inf]\n",
    "plot_binned_RMSE(dst_test_1_min.loc[valid_ind, \"dst\"].values, dst_test_1_min.loc[valid_ind, \"prediction_t\"].values, bins)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Size of <i>Dst</i> ranges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bin_id = pd.cut(dst_test_1_min.loc[valid_ind, \"dst\"], bins).to_frame(\"bin\")\n",
    "bin_id.groupby(\"bin\").size().to_frame(\"size\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploring the performance at storm events\n",
    "\n",
    "Here we plot some graphs showing how the model performs during storm events.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot forecast vs actual at storm events"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select events at least 7 days apart\n",
    "num_events = 10\n",
    "min_ind_arr = []\n",
    "min_time_arr = []\n",
    "labels = []\n",
    "# dst\n",
    "dst_test['exclude'] = False\n",
    "for i in range(num_events):\n",
    "    min_ind = dst_test.loc[~dst_test['exclude'], 'dst'].idxmin()\n",
    "    min_time = dst_test.loc[min_ind, ['timedelta', 'period']]\n",
    "    min_time_arr.append(min_time)\n",
    "    t, p = min_time['timedelta'], min_time['period']\n",
    "    min_ind_arr.append(min_ind)\n",
    "    labels.append(f\"{min_time['period'], min_time['timedelta']}\")\n",
    "    dst_test['exclude'] = dst_test['exclude'] | (((dst_test['timedelta'] - t).dt.total_seconds().abs() <= 7 * 24 * 3600) & (dst_test['period'] == p))\n",
    "\n",
    "# sort by period and timedelta\n",
    "sort_ind = list(range(num_events))\n",
    "sort_ind = sorted(sort_ind, key=lambda x: (min_time_arr[x]['period'], min_time_arr[x]['timedelta']))\n",
    "min_ind_arr = [min_ind_arr[i] for i in sort_ind]\n",
    "labels = [labels[i] for i in sort_ind]\n",
    "\n",
    "\n",
    "for i in range(num_events):\n",
    "    # extract 96 hours before and after max\n",
    "    ind = min_ind_arr[i]\n",
    "    # centre on min within 96 * 2 hour window\n",
    "    new_min = dst_test.loc[ind - 96: ind + 96, 'dst'].idxmin()\n",
    "    df = dst_test.loc[new_min - 96: new_min + 96].copy()\n",
    "    df = pd.merge(df, dst_test_1_min[[\"period\", \"timedelta\", \"prediction_t\"]], how=\"left\", on=[\"timedelta\", \"period\"])\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.plot(df[\"dst\"].values, c=\"black\")\n",
    "    plt.plot(df[\"prediction_t\"].values, c=\"blue\")\n",
    "    plt.legend([\"dst\", \"prediction_t\"])\n",
    "    plt.title(labels[i])\n",
    "    plt.xlabel(\"hours\")\n",
    "    plt.ylabel(\"Dst\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29cd44bc"
   },
   "outputs": [],
   "source": [
    ""
   ],
   "id": "29cd44bc"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "magnet_cnn_tutorial.ipynb",
   "provenance": [],
   "machine_shape": "hm",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}