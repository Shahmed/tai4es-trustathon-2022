{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKpOdQvxbeB4"
   },
   "source": [
    "# MagNet: Model the Geomagnetic Field:\n",
    "This notebook provides a TAI4ES Summer School learning journey using the benchmark LSTM model from the NOAA MagNet challenge. You'll want to open up our space weather use case README as well as sibling notebook magnet_cnn_tutorial.ipynb inside your prepared environment. You can find the latest materials in our [space weather TAI4ES github folder](https://github.com/ai2es/tai4es-trustathon-2022/tree/main/space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Learn about ML modeling a key space weather storm indicator <i>Dst</i>\n",
    "\n",
    "The overall goal is to learn about ML modeling a key space weather storm indicator, the disturbance storm-time (<i>Dst</i>) index.\n",
    "\n",
    "We will use two space weather <i>Dst</i> notebooks throughout your summer school experience:\n",
    "* magnet_lstm_tutorial.ipynb - ideal for students newer to AI/ML\n",
    "* magnet_cnn_tutorial.ipynb - ideal for students with more AI/ML experience\n",
    "\n",
    "Use the space weather README to guide your experience.\n",
    "\n",
    "In this notebook, you'll notice color-coded boxes, which may provide optional tasks, questions to discuss, or warnings. Here is the color-coding breakdown: \n",
    "* <span style=\"color:blue\">Blue Box</span> = Question to discuss\n",
    "* <span style=\"color:red\">Red Box</span> = Warning/Caution\n",
    "* <span style=\"color:gold\">Yellow Box</span> = Optional task to perform \n",
    "* <span style=\"color:green\">Green Box</span> = Useful Tip or Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRzCSuRwbeB7"
   },
   "source": [
    "## Background on Geospace Space Weather\n",
    "\n",
    "Just like the terrestrial weather we are used to experiencing in our daily lives, weather also occurs in the space environment. If you'd like a general primer on space weather and it's effects on the technological systems we rely on, check out [NASA's Space Place](https://spaceplace.nasa.gov/spaceweather/), as well as [NOAA's Space Weather Prediction Center (SWPC)](https://www.swpc.noaa.gov/), in particular their community dashboards.\n",
    "\n",
    "![HELIO_GRAPHIC_URL](https://ngdc.noaa.gov/geomag/img/challenge-banner.png \"HELIO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIeZxbJXbeB7"
   },
   "source": [
    "## Background on the Geomagnetic Field\n",
    "\n",
    "The efficient transfer of energy from solar wind into the Earth’s magnetic field causes geomagnetic storms. The resulting variations in the magnetic field increase errors in magnetic navigation. The disturbance-storm-time index, or <i>Dst</i>, is a measure of the severity of the geomagnetic storm.\n",
    "\n",
    "As a key specification of the magnetospheric dynamics, the <i>Dst</i> index is used to drive geomagnetic disturbance models such as NOAA/NCEI’s High Definition Geomagnetic Model - Real-Time (HDGM-RT).\n",
    "![HDGMRT_GRAPHIC_URL](https://www.ngdc.noaa.gov/geomag/HDGM/images/HDGM-RT_2003_storm_720p.gif \"HDGM-RT\")\n",
    " \n",
    "In 2020-2021, NOAA and NASA conducted an international crowd sourced data science competition “MagNet: Model the Geomagnetic Field”:\n",
    "https://www.drivendata.org/competitions/73/noaa-magnetic-forecasting/\n",
    " \n",
    "Empirical models have been proposed as early as in 1975 to forecast <i>Dst</i> solely from solar-wind observations at the Lagrangian (L1) position by satellites such as NOAA’s Deep Space Climate Observatory (DSCOVR) or NASA's Advanced Composition Explorer (ACE). Over the past three decades, several models were proposed for solar wind forecasting of <i>Dst</i>, including empirical, physics-based, and machine learning approaches. While the ML models generally perform better than models based on the other approaches, there is still room to improve, especially when predicting extreme events. More importantly, we intentionally sought solutions that work on the raw, real-time data streams and are agnostic to sensor malfunctions and noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdZDisojbeB8"
   },
   "source": [
    "## Modeling Task\n",
    "\n",
    "The MagNet competition task was to develop models for forecasting <i>Dst</i> that push the boundary of predictive performance, under operationally viable constraints, using the real-time solar-wind (RTSW) data feeds from NOAA’s DSCOVR and NASA’s ACE satellites. Improved models can provide more advanced warning of geomagnetic storms and reduce errors in magnetic navigation systems. Specifically, given one week of data ending at t minus 1 minute, the model must forecast <i>Dst</i> at time t and t plus one hour.\n",
    "\n",
    "The model described in this notebook is the benchmark model provided by the MagNet competition organizers. Long Short Term Memory networks or LSTMs are a special kind of recurrent neural network especially suited to time series data. In the related notebook, we will show you how to implement a first-pass LSTM model for predicting <i>Dst</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question: </b> Can you describe the physical process between solar wind and ground geomagnetic disturbances? What is the <i>Dst</i> index primarily used for?\n",
    "Roughly 85% of the time, near Earth is geomagnetically quiet. How might these infrequent solar wind events make modeling their predicted effects challenging? How might you make an accurate model with very few extreme events/samples?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i>You can use this empty cell to write down your thoughts regarding the preceding question(s).</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Notes\n",
    "\n",
    "The target <i>Dst</i> values are measured by 4 ground-based observatories near the equator. These values are then averaged to provide a measurement of <i>Dst</i> for any given hour.\n",
    "To ensure similar distributions between the training and test data, the data is separated into three non-contiguous periods. All data are provided with a `period` and `timedelta` multi-index which indicates the relative timestep for each observation within a period, but not the real timestamp. The period identifiers and timedeltas are common across datasets. Converting back from our index date and time to real geophysical date and time as simple as adding the the start date/time in the table below to the relative timestep provided with the data.\n",
    "\n",
    "Table: Dataset Period Time Ranges\n",
    "\n",
    "| Period  | Beginning               | End                      |\n",
    "|---------|-------------------------|--------------------------|\n",
    "| train_a | 1998, 2, 16, '00:00:00' | 2001, 5, 31, '23:59:00'  |\n",
    "| train_b | 2013, 6, 1, '00:00:00'  | 2019, 5, 31, '23:59:00'  |\n",
    "| train_c | 2004, 5, 1, '00:00:00'  | 2010, 12, 31, '23:59:00' |\n",
    "|  test_a | 2001, 6, 1, '00:00:00'  | 2004, 4, 30, '23:59:00'  |\n",
    "|  test_b | 2010, 1, 1, '00:00:00'  | 2013, 5, 31, '23:59:00'  |\n",
    "|  test_c | 2017, 6, 1, '00:00:00'  | 2020, 10, 31, '23:00:00' |\n",
    "\n",
    "\n",
    "![Figure_Activity_and_Training_Splits.png](https://github.com/ai2es/tai4es-trustathon-2022/raw/space/space/notebook_figures/Figure_Activity_and_Training_Splits.png)\n",
    "<i>Figure: Plot shows solar activity as the sunspot number (SSN) (orange), the geomagnetic storm index Dst (blue), and the public and private data segments (red shaded).  SSN and Dst have been normalized. The time range shown is January 1998 through December 2022. This figure and the table preceeding it have been adapted from Nair et al. (manuscript in preparation).</i>\n",
    "\n",
    "The competitors used the training part (“train_a”,”train_b” and “train_c”) data to develop and improve their models. When they submitted a model, the competition platform used the test data sets (“test_a”,”test_b” and “test_c”)  to calculate the accuracy of the model. The model evaluation was done separately for a public leaderboard and for a private leaderboard. The public leaderboard was openly accessible whereas the private leaderboard was restricted to the competition administrators. The  data from all of the training sets (a, b, and c) were used on the public leaderboard and private leaderboard. We randomly sampled rows to be included in the public and private leaderboard. Based on relative performance from the public leaderboard as a clue, the teams iterated their models. The final ranking of the models was done on the private leaderboard.\n",
    "\n",
    "### Input (feature) data sources - there are 3:\n",
    "* Satellite measurements of the solar wind, including direction, speed, density and temperature, at 1-minute cadence.\n",
    "* Position of the satellite used for solar wind measurements. The ACE and DSCOVR satellites are positioned just outside Earth's exosphere approximately 1% of the distance from Earth to Sun. As noted above, this is referred to as the Sun Earth L1 position.\n",
    "* Number of sunspots on the Sun, measured monthly.\n",
    "\n",
    "Here is a description of several of these inputs (features) observed by the ACE or DSCOVR satellites (see [here](https://www.drivendata.org/competitions/73/noaa-magnetic-forecasting/page/279/) for the full list):\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "* bt - Interplanetary-magnetic-field magnitude (nT)\n",
    "* bx_gsm - Interplanetary-magnetic-field X-component in geocentric solar magnetospheric (GSM) coordinate (nT)\n",
    "* by_gsm - Interplanetary-magnetic-field Y-component in GSM coordinate (nT)\n",
    "* bz_gsm - Interplanetary-magnetic-field Z-component in (GSM) coordinate (nT)\n",
    "* density - Solar wind proton density (N/cm^3)\n",
    "* speed - Solar wind bulk speed (km/s) flowing from Sun to Earth\n",
    "* temperature - Solar wind ion temperature (Kelvin)\n",
    "</div>\n",
    "\n",
    "To get a feeling for the GSM coordinate reference frame:\n",
    "\n",
    "The X-axis is oriented from the Earth to the Sun. The positive Z-axis is chosen to be in the same sense as the northern magnetic pole. And the Y-axis is defined to be perpendicular to the Earth's magnetic dipole so that the X-Z plane contains the dipole axis. For additional details, see [here](https://www.spenvis.oma.be/help/background/coortran/coortran.html#GSM).\n",
    "\n",
    "To see how several of these parameter look during an example space weather event see [Figure 5](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2018SW001897#swe20716-fig-0005) of Redmon et al., 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Acknowledgements:\n",
    "\n",
    "Author: Rob Redmon\n",
    "\n",
    "Points of contact: Rob.Redmon@noaa.gov, Manoj.C.Nair@noaa.gov\n",
    "\n",
    "Topics on Explainable AI (XAI) (e.g. Feature Importance, Case Studies) are new and were created for the NCAR and [AI2ES](https://www.ai2es.org/) Trustworthy Artificial Intelligence for Environmental Science Summer School ([TAI4ES](https://www2.cisl.ucar.edu/events/tai4es-2022-summer-school)). Code through model training adapted from [DrivenData's blogpost](https://www.drivendata.co/blog/model-geomagnetic-field-benchmark/) supporting [NOAA's MagNet crowd sourced challenge](https://ngdc.noaa.gov/geomag/mag-net-challenge.html). \n",
    "\n",
    "Introductory documentation has been aligned with and adapted from this LSTM notebook's sibling ensemble CNN notebook [magnet_cnn_tutorial.ipynb](https://github.com/liyo6397/MagNet/blob/master/magnet_cnn_tutorial.ipynb). For additional outside See this overall MagNet repository for additional architectures including the MagNet challenge winning solutions and other helpful materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Acquire and Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Essential Modules and Basic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#%load_ext nb_black                # Nice for iPython, not available for Colab.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#%matplotlib inline                # For iPython\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np, pandas as pd, pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%load_ext nb_black                # Nice for iPython, not available for Colab.\n",
    "#%matplotlib inline                # For iPython\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Matplotlib Configuration\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Summary\n",
    "The competition discussed above used <i>public</i> data for development and the public leaderboard. A <i>private</i> dataset was kept internal during the competition for use in scoring by organizers. Since the competition has passed, both datasets are publicly accessible from NOAA. We will build and evaulate the model using the competition's <i>public</i> data and evaluate storm event case studies using the competition's <i>private</i> data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Retrieve Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture captured_io\n",
    "%%bash\n",
    "\n",
    "# Download data we need. If a directory \"data/\" already exists, we'll assume the data are already downloaded.\n",
    "#      The above \"magic\" statements are used to capture shell in/out and to run the following Bash commands.\n",
    "if [ ! -d \"data\" ]; then\n",
    "  wget --verbose https://ngdc.noaa.gov/geomag/data/geomag/magnet/public.zip\n",
    "  wget --verbose https://ngdc.noaa.gov/geomag/data/geomag/magnet/private.zip\n",
    "  unzip public.zip\n",
    "  unzip private.zip\n",
    "  mkdir -v data\n",
    "  mv -v public private data/\n",
    "  mv -v public.zip private.zip data/\n",
    "fi\n",
    "# Uncomment for debugging if you have trouble downloading:\n",
    "#print(captured_io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import  Input (Features) and Output (Labels) as Pandas DataFrames\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "As described above, the input data is a time series of solar wind measurements at L1 along with sunspot number, and the output data is a time series of <i>Dst</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import as Pandas DataFrames\n",
    "DATA_PATH = Path(\"data/public/\")\n",
    "\n",
    "dst = pd.read_csv(DATA_PATH / \"dst_labels.csv\")\n",
    "dst.timedelta = pd.to_timedelta(dst.timedelta)\n",
    "dst.set_index([\"period\", \"timedelta\"], inplace=True)\n",
    "\n",
    "sunspots = pd.read_csv(DATA_PATH / \"sunspots.csv\")\n",
    "sunspots.timedelta = pd.to_timedelta(sunspots.timedelta)\n",
    "sunspots.set_index([\"period\", \"timedelta\"], inplace=True)\n",
    "\n",
    "solar_wind = pd.read_csv(DATA_PATH / \"solar_wind.csv\")\n",
    "solar_wind.timedelta = pd.to_timedelta(solar_wind.timedelta)\n",
    "solar_wind.set_index([\"period\", \"timedelta\"], inplace=True)\n",
    "\n",
    "satellite_positions = pd.read_csv(DATA_PATH / \"satellite_positions.csv\")\n",
    "satellite_positions.timedelta = pd.to_timedelta(satellite_positions.timedelta)\n",
    "satellite_positions.set_index([\"period\", \"timedelta\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Exploration\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "We'll explore our input (feature) and output (label) data to better understand it's data architecture, statistical description and basic input-output relationships.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Disturbance Storm-Time Index (<i>Dst</i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The structure of our output (label) data, Dst time series:\n",
    "print(\"Dst shape: \", dst.shape)\n",
    "dst.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dst.groupby(\"period\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize the difference between the periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "colors = [\"r\", \"b\", \"y\"]\n",
    "for i, period in enumerate(dst.groupby(\"period\")):\n",
    "    period_name, df = period\n",
    "    # FYI: For Google Colaborory (Python 3.7, Matplotlib 3.2.2), in the call to ax.hist(), you need to transpose the DataFrame.\n",
    "    #      For Python 3.8+ (e.g. 3.9.7, with Matplotlib 3.5.1) it seems no tranpose is needed, thus this version switch.\n",
    "    #      Below we differentiate on Python version, while you might want to differentiate on Matplotlib version.\n",
    "    #      Error message you might see if DataFrame isn't oriented correctly: \n",
    "    #           \"ValueError: color kwarg must have one color per data set. 28824 data sets and 1 colors were provided\"\n",
    "    import sys\n",
    "    if float(sys.version[:3]) <= 3.7:\n",
    "        ax.hist(df.T, alpha=0.5, color=colors[i], bins=50, label=period_name)\n",
    "    else:\n",
    "        ax.hist(df,   alpha=0.5, color=colors[i], bins=50, label=period_name)\n",
    "    ax.set_xlim([-100,50])\n",
    "    ax.set_xlabel('Dst')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of Dst values across Periods\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Solar Wind and Sunspots\n",
    "This is our time series input (feature) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Solar wind shape: \", solar_wind.shape)\n",
    "solar_wind.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Sunspot shape: \", sunspots.shape)\n",
    "sunspots.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "solar_wind.groupby(\"period\").describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sunspots.groupby(\"period\").describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_raw_visualization(data):\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 20), dpi=80)\n",
    "    for i, key in enumerate(data.columns):\n",
    "        t_data = data[key]\n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i // 2, i % 2],\n",
    "            title=f\"{key.capitalize()}\",\n",
    "            rot=25,\n",
    "        )\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.8)\n",
    "    plt.tight_layout()\n",
    "\n",
    "cols_to_plot = [\"bx_gsm\", \"by_gsm\", \"bz_gsm\", \"bt\", \"speed\", \"density\", \"temperature\", \"theta_gsm\"]\n",
    "show_raw_visualization(solar_wind[cols_to_plot].iloc[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data gaps in the Solar Wind data are a common issue with real-time data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Gaps in our input (features) are something we'll need to deal carefully with, i.e. in the preprocessing steps below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# What percent of the time are there gaps in these data?\n",
    "print('Data gaps in solar input (features) as % of data:')\n",
    "solar_wind.isna().sum()/len(solar_wind)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question: </b>What challenges in working with these \"operational\" observations of the solar wind will we need to solve before modeling? What complexities of the data will need to be captured and solved by the model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i>You can use this empty cell to write down your thoughts regarding the preceding question(s).</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation matrix:\n",
    "#     Note that this is a slow command (several minutes) unless you have a GPU or TPU equivalent processor (then it's ~1 min).\n",
    "#     Take advantage of Pandas DataFrame and merge our Input (Feature) and Output (Label) data.\n",
    "#     I.e. merge, Solar Wind + Sunspots + Satellite Location + Dst\n",
    "corr = solar_wind.join(sunspots).join(satellite_positions).join(dst).fillna(method=\"ffill\").corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.matshow(corr, cmap='seismic', vmin=-1, vmax=1, fignum=1)\n",
    "plt.xticks(range(corr.shape[1]), corr.columns, rotation=90)\n",
    "plt.gca().xaxis.tick_bottom()\n",
    "plt.yticks(range(corr.shape[1]), corr.columns)\n",
    "\n",
    "\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title(\"Feature Correlation Heatmap\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question: </b>Evaluate the feature correlation heatmap figure. What input (features) are correlated with each other and which ones are correlated with our output <i>Dst</i>? Does satellite location seem to be important or not? You might like to consult the input parameter descriptions in the \"Data Notes\" section near the top of this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i>You can use this empty cell to write down your thoughts regarding the preceding question(s).</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "seed(2020)\n",
    "set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature / Input Data we'll use to Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Optional Task: </b> What features would you recommend for use in developing our model? What additional considerations should we evaluate before using our chosen features to develop a model? Are there any additional features we should look at that are not in our list? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Optional Task: </b> Use a different list of features and evaluate the trade offs and your model's performance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# subset of solar wind features to use for modeling\n",
    "SOLAR_WIND_FEATURES = [\n",
    "    \"bt\",\n",
    "    \"temperature\",\n",
    "    \"bx_gsm\",\n",
    "    \"by_gsm\",\n",
    "    \"bz_gsm\",\n",
    "    \"speed\",\n",
    "    \"density\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The model will be built on feature statistics, mean and standard deviation\n",
    "XCOLS = (\n",
    "    [col + \"_mean\" for col in SOLAR_WIND_FEATURES]\n",
    "    + [col + \"_std\" for col in SOLAR_WIND_FEATURES]\n",
    "    + [\"smoothed_ssn\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Preparing Inputs (Features): </b>As discussed above, we'll need to fill in gaps and create statistical summaries (hourly means and standard deviations) of our features before modeling. The following routines provide this \"preprocessing\" functionality of gap filling, and scaling by features' statistics.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Optional Task: </b> In this LSTM notebook, we use means and standard deviations. Consider and perhaps try different statistical tools such as medians and inner quartile ranges, such as used in our sibling CNN notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def impute_features(feature_df):\n",
    "    \"\"\"Imputes data using the following methods:\n",
    "    - `smoothed_ssn`: forward fill\n",
    "    - `solar_wind`: interpolation\n",
    "    \"\"\"\n",
    "    # forward fill sunspot data for the rest of the month\n",
    "    feature_df.smoothed_ssn = feature_df.smoothed_ssn.fillna(method=\"ffill\")\n",
    "    # interpolate between missing solar wind values\n",
    "    feature_df = feature_df.interpolate()\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "def aggregate_hourly(feature_df, aggs=[\"mean\", \"std\"]):\n",
    "    \"\"\"Aggregates features to the floor of each hour using mean and standard deviation.\n",
    "    e.g. All values from \"11:00:00\" to \"11:59:00\" will be aggregated to \"11:00:00\".\n",
    "    \"\"\"\n",
    "    # group by the floor of each hour use timedelta index\n",
    "    agged = feature_df.groupby(\n",
    "        [\"period\", feature_df.index.get_level_values(1).floor(\"H\")]\n",
    "    ).agg(aggs)\n",
    "    # flatten hierachical column index\n",
    "    agged.columns = [\"_\".join(x) for x in agged.columns]\n",
    "    return agged\n",
    "\n",
    "\n",
    "def preprocess_features(solar_wind, sunspots, scaler=None, subset=None):\n",
    "    \"\"\"\n",
    "    Preprocessing steps:\n",
    "        - Subset the data\n",
    "        - Aggregate hourly\n",
    "        - Join solar wind and sunspot data\n",
    "        - Scale using standard scaler\n",
    "        - Impute missing values\n",
    "    \"\"\"\n",
    "    # select features we want to use\n",
    "    if subset:\n",
    "        solar_wind = solar_wind[subset]\n",
    "\n",
    "    # aggregate solar wind data and join with sunspots\n",
    "    hourly_features = aggregate_hourly(solar_wind).join(sunspots)\n",
    "\n",
    "    # subtract mean and divide by standard deviation\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(hourly_features)\n",
    "\n",
    "    normalized = pd.DataFrame(\n",
    "        scaler.transform(hourly_features),\n",
    "        index=hourly_features.index,\n",
    "        columns=hourly_features.columns,\n",
    "    )\n",
    "\n",
    "    # impute missing values\n",
    "    imputed = impute_features(normalized)\n",
    "\n",
    "    # we want to return the scaler object as well to use later during prediction\n",
    "    return imputed, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features, scaler = preprocess_features(solar_wind, sunspots, subset=SOLAR_WIND_FEATURES)\n",
    "print(features.shape)\n",
    "features.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check to make sure missing values are filled\n",
    "assert (features.isna().sum() == 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Preparing <i>Dst</i>: </b>We also need to prepare our output (labels), i.e. our space weather storm index <i>Dst</i>, which is already a time series with an hourly cadence. The modelling task is to predict <i>Dst</i> at hour t0 and the next hour t1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "YCOLS = [\"t0\", \"t1\"]\n",
    "\n",
    "\n",
    "def process_labels(dst):\n",
    "    y = dst.copy()\n",
    "    y[\"t0\"] = y.groupby(\"period\").dst.shift(-1)\n",
    "    y[\"t1\"] = y.groupby(\"period\").dst.shift(-2)\n",
    "    return y[YCOLS]\n",
    "\n",
    "\n",
    "labels = process_labels(dst)\n",
    "labels.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Features + Labels </b>: For convenience, join our processed solar wind hourly inputs and our <i>Dst</i> labels into one DataFrame.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = labels.join(features)\n",
    "data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Splitting the Data\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "We'll split our features and labels into Training, Testing and Validation sets for each of the 3 training periods, named train_a, train_b, train_c (see Data Notes for additional details).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSdIiKQLOOEQ"
   },
   "outputs": [],
   "source": [
    "def get_train_test_val(data, test_per_period, val_per_period):\n",
    "    \"\"\"Splits data across periods into train, test, and validation\"\"\"\n",
    "    # assign the last `test_per_period` rows from each period to test\n",
    "    test = data.groupby(\"period\").tail(test_per_period)\n",
    "    interim = data[~data.index.isin(test.index)]\n",
    "    # assign the last `val_per_period` from the remaining rows to validation\n",
    "    val = interim.groupby(\"period\").tail(val_per_period)\n",
    "    # the remaining rows are assigned to train\n",
    "    train = interim[~interim.index.isin(val.index)]\n",
    "    return train, test, val\n",
    "\n",
    "\n",
    "train, test, val = get_train_test_val(data, test_per_period=6_000, val_per_period=3_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqKPO3qXOWDb"
   },
   "source": [
    "## Visualize splits - Counts across periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8mapzk9eOYMt",
    "outputId": "637f0e0c-2e1e-4fe8-fe79-4b6fae394f2c"
   },
   "outputs": [],
   "source": [
    "ind = [0, 1, 2]\n",
    "names = [\"train_a\", \"train_b\", \"train_c\"]\n",
    "width = 0.75\n",
    "train_cnts = [len(df) for _, df in train.groupby(\"period\")]\n",
    "val_cnts = [len(df) for _, df in val.groupby(\"period\")]\n",
    "test_cnts = [len(df) for _, df in test.groupby(\"period\")]\n",
    "\n",
    "p1 = plt.barh(ind, train_cnts, width)\n",
    "p2 = plt.barh(ind, val_cnts, width, left=train_cnts)\n",
    "p3 = plt.barh(ind, test_cnts, width, left=np.add(val_cnts, train_cnts).tolist())\n",
    "\n",
    "plt.yticks(ind, names)\n",
    "plt.ylabel(\"Period\")\n",
    "plt.xlabel(\"Hourly Timesteps\")\n",
    "plt.title(\"Train/Validation/Test Splits over Periods\", fontsize=16)\n",
    "plt.legend([\"Train\", \"Validation\", \"Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_islsePpOgRI"
   },
   "source": [
    "## Final look at each of our \"Training\", \"Test\", and \"Validation\" datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0zdssN9POjE4",
    "outputId": "195752b3-334e-40a5-d601-5386684c5c1b"
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DdVye7Q6OuW5",
    "outputId": "5641829a-e0d8-45ca-a725-d10292bc84bb"
   },
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XDWiaLh0OwQb",
    "outputId": "8be02465-d16d-45b5-e9d5-7d822ab9792d"
   },
   "outputs": [],
   "source": [
    "print(val.shape)\n",
    "val.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4FM-_JaTygx"
   },
   "source": [
    "# Load a Pre-Trained Model - <i>Optional</i>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Optional Task: </b>You can use the following cells to load a pre-trained model, or you can skip ahead and train a new model yourself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_model = True     # Set True to skip next section and create a new model.\n",
    "                            # Set False to load a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If Pre-existing then load Model, Scaler, History and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpYjboipHS0w"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "import glob\n",
    "# List existing LSTM models:\n",
    "dir_list = glob.glob('trained_models_lstm/model_lstm_*/')\n",
    "print('Here is a list of pre-trained models:\\n')\n",
    "for i in range(len(dir_list)):\n",
    "    print('    %d: %s' % (i, dir_list[i]))\n",
    "\n",
    "if not create_new_model:\n",
    "    dir_model = dir_list[int(input('Enter number of pre-trained model: '))]\n",
    "    \n",
    "    import json\n",
    "    import pickle\n",
    "    \n",
    "    # Load in serialized model, config, and scaler\n",
    "    print('Loading pre-trained model from: %s' % dir_model)\n",
    "    model = keras.models.load_model(dir_model)\n",
    "    model.summary()\n",
    "\n",
    "    # Load Scaler\n",
    "    with open(dir_model+\"/scaler.pck\", \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "    print('Scaler:')\n",
    "    pprint.pprint(scaler)\n",
    "\n",
    "    # Load History\n",
    "    with open(dir_model+\"/history.pck\", \"rb\") as f:\n",
    "        history = pickle.load(f)\n",
    "    print('History:')\n",
    "    pprint.pprint(history)\n",
    "\n",
    "    # Load Configuration\n",
    "    with open(dir_model+\"/config.json\", \"r\") as f:\n",
    "        data_config = json.load(f)\n",
    "    print('Configuration:')\n",
    "    pprint.pprint(data_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y07bR5JTPwra"
   },
   "source": [
    "# Define and Build our LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHXdMwzzTHaF"
   },
   "source": [
    "## Define LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aMIfKprbeCI"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Optional Task (highly recommended): </b>The initial LSTM hyper parameters are set for speed. Load a pre-trained model or adjust these yourself and evaluate consequences on model convergence and storm event use case performance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C966_JivTtcJ",
    "outputId": "cd0e6718-df46-4be7-fa89-8fb6039d8f82"
   },
   "outputs": [],
   "source": [
    "# If we're Defining and Training a New Model\n",
    "if create_new_model:\n",
    "    from keras.layers import Dense, LSTM\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    # Define our model\n",
    "    data_config = {\n",
    "        \"timesteps\": 32,\n",
    "        \"batch_size\": 32,\n",
    "    }\n",
    "    print('data_config = ')\n",
    "    pprint.pprint(data_config)\n",
    "\n",
    "    # Hyper Parameter Tuning\n",
    "    #\n",
    "    # Going Big (takes hours):\n",
    "    #      model_config = {\"n_epochs\": 30, \"n_neurons\": 2048, \"dropout\": 0.4, \"stateful\": False}\n",
    "    #\n",
    "    # Original from MagNet blogpost benchmark, takes about 1.5 hours:\n",
    "    #      model_config = {\"n_epochs\": 20, \"n_neurons\": 512, \"dropout\": 0.4, \"stateful\": False}\n",
    "    #\n",
    "    # Takes 10-15 minutes (moderate performance): \n",
    "    #      model_config = {\"n_epochs\": 8, \"n_neurons\": 64, \"dropout\": 0.4, \"stateful\": False}\n",
    "    #\n",
    "    # Takes 20 seconds (anticipate bad performance):\n",
    "    model_config = {\"n_epochs\": 4, \"n_neurons\": 16, \"dropout\": 0.4, \"stateful\": False}\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            model_config[\"n_neurons\"],\n",
    "            # usually set to (`batch_size`, `sequence_length`, `n_features`)\n",
    "            # setting the batch size to None allows for variable length batches\n",
    "            batch_input_shape=(None, data_config[\"timesteps\"], len(XCOLS)),\n",
    "            stateful=model_config[\"stateful\"],\n",
    "            dropout=model_config[\"dropout\"],\n",
    "        )\n",
    "    )\n",
    "    model.add(Dense(len(YCOLS)))\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=\"adam\",\n",
    "        run_eagerly=None,     # set to True for debugging (very slow), None or False\n",
    "    )\n",
    "\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4FM-_JaTygx"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchDataset: Training, Validation and Test Data\n",
    "In order to train and/or test our New or Pre-trained model, we'll create <i>[tensorflow.python.data.ops.dataset_ops.BatchDataset](https://www.tensorflow.org/guide/data#batching_dataset_elements)</i> structures for our Training, Validation and Test DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XD7CJDs0PzQ4",
    "outputId": "ac8ec3b7-dded-4f67-c885-41a4436100dc"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras import preprocessing\n",
    "\n",
    "def timeseries_dataset_from_df(df, batch_size):\n",
    "    dataset = None\n",
    "    timesteps = data_config[\"timesteps\"]\n",
    "\n",
    "    # iterate through periods\n",
    "    for _, period_df in df.groupby(\"period\"):\n",
    "        # realign features and labels so that first sequence of 32 is aligned with the 33rd target\n",
    "        inputs = period_df[XCOLS][:-timesteps]\n",
    "        outputs = period_df[YCOLS][timesteps:]\n",
    "\n",
    "        period_ds = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            inputs,\n",
    "            outputs,\n",
    "            timesteps,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        if dataset is None:\n",
    "            dataset = period_ds\n",
    "        else:\n",
    "            dataset = dataset.concatenate(period_ds)\n",
    "        \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = timeseries_dataset_from_df(train, data_config[\"batch_size\"])\n",
    "val_ds   = timeseries_dataset_from_df(val,   data_config[\"batch_size\"])\n",
    "test_ds  = timeseries_dataset_from_df(test,  data_config[\"batch_size\"])\n",
    "\n",
    "print(f\"Number of training batches: {len(train_ds)}\")\n",
    "print(f\"Number of validation batches: {len(val_ds)}\")\n",
    "print(f\"Number of test batches: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4FM-_JaTygx"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Take a break: </b>Depending on the settings you chose above, you might go grab a snack or a cup of coffee/tea. If your model is likely to take several hours to train, perhaps open another copy of this notebook so you can work in parallel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MWiq8-BDTyz5",
    "outputId": "20c8f172-75e4-4d9f-cd58-9eaf658ac92f"
   },
   "outputs": [],
   "source": [
    "# If we're training a New model:\n",
    "if create_new_model:\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        batch_size=data_config[\"batch_size\"],\n",
    "        epochs=model_config[\"n_epochs\"],\n",
    "        verbose=True,\n",
    "        shuffle=False,\n",
    "        validation_data=val_ds,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkwxYtOqT8WT"
   },
   "source": [
    "## Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "87GD27_tT-JY",
    "outputId": "74efa4d9-faab-4821-e277-a77dd0a3dccd"
   },
   "outputs": [],
   "source": [
    "for name, values in history.history.items():\n",
    "    plt.plot(values, label=name)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvj69RaTUoBw",
    "outputId": "4be40542-0477-46bd-a90b-c12da99ee3e6"
   },
   "outputs": [],
   "source": [
    "rmse = model.evaluate(test_ds)**0.5\n",
    "print(f\"Test RMSE: {rmse:.2f} nano-Tesla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question: </b>Did the model converge, ie. how flat are the loss curves above? What hyper parameters might you tune?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i>You can use this empty cell to write down your thoughts regarding the preceding question(s).</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jsey4js4HMK4"
   },
   "source": [
    "# Save and Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Model and Scaler: </b>Recall that we used a scaler (mean and standard deviation) to scale the features. We'll need to save both the trained model and scaler if we'd like to use it again in a future Jupyter session without having to retrain.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl-bgU9WHXQO"
   },
   "source": [
    "## Save: New Model, Scaler, History and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRJVvgXDHPJh",
    "outputId": "ee234357-8983-4122-a383-d997a35b30c2"
   },
   "outputs": [],
   "source": [
    "if create_new_model:\n",
    "    # Pickle the Scaler and History, and JSON the Config\n",
    "    import json\n",
    "    import pickle\n",
    "\n",
    "    # Keep our models in their own subdirectories\n",
    "    dir_model = 'trained_models_lstm/model_lstm_nepochs-%02d_nneurons-%04d/' % \\\n",
    "        (model_config['n_epochs'], model_config['n_neurons'])\n",
    "\n",
    "    # Save Model\n",
    "    model.save(dir_model)\n",
    "\n",
    "    # Save Scaler (pickle)\n",
    "    with open(dir_model+\"/scaler.pck\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Save History (pickle)\n",
    "    with open(dir_model+\"/history.pck\", \"wb\") as f:\n",
    "        pickle.dump(history, f)\n",
    "\n",
    "    # Save Configuration (as JSON)\n",
    "    data_config[\"solar_wind_subset\"] = SOLAR_WIND_FEATURES\n",
    "    with open(dir_model+\"/config.json\", \"w\") as f:\n",
    "        json.dump(data_config, f)\n",
    "\n",
    "    print('Saved model to %s' % dir_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2o3MHneUD0M"
   },
   "source": [
    "# Explainable AI (XAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxmhOeEBNnE2"
   },
   "source": [
    "## Permutation Importance - Easy Approximation\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Based on Christoph Molnar's \"Interpretable Machine Learning\" section and Fisher, Rudin, and Dominici (2018), we will \"split the dataset in half and swap the values of feature j of the two halves instead of permuting feature j\".     \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "*   Christoph Molnar's \"Interpretable Machine Learning\" section on [Permutation Feature Importance](https://christophm.github.io/interpretable-ml-book/feature-importance.html), and see also their <b>argument for using Test data</b> for Permutation Importance evaluation, which we have chosen to do here.\n",
    "*   [See this illustrative graphic demonstrating single- and multi-pass Permutation Importance](https://permutationimportance.readthedocs.io/en/latest/methods.html#permutation-importance)\n",
    "*   [Permutation Feature Importance in the <i>scikit-learn</i> module](https://scikit-learn.org/stable/modules/permutation_importance.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we can split and swap the feature datasets one feature at a time and compare the resultant RMSE. We take a programming convenience shortcut and <b>simply reverse each feature</b> vector rather than split and swap and we expect the same results. We'll do this, i.e. permute each feature vector, one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our <i>test_ds</i> which we used to evaluate the model performance is a [tensorflow.python.data.ops.dataset_ops.BatchDataset](https://www.tensorflow.org/guide/data#batching_dataset_elements) and these are honestly kind of hard to work with. So we will recreate a deep copy of <i>test_ds</i> for each permutation and so we don't corrupt the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhBr9xlT3IdZ"
   },
   "outputs": [],
   "source": [
    "# Note: We shouldn't need these two lines below but they seem needed generalizing to run w/o\n",
    "#       issues on both Colaboratory (Python 3.7) and Jupyter server with Python 3.9.\n",
    "#       Contact POCs if you get an error such as:\n",
    "#            AttributeError: module 'keras.preprocessing' has no attribute 'timeseries_dataset_from_array'\n",
    "import tensorflow.keras as keras\n",
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K92V7O4cPCiK",
    "outputId": "1b3467f8-9339-47d6-c9f2-47de3139b988"
   },
   "outputs": [],
   "source": [
    "# A couple of ways to learn about the contents of a BatchDataset:\n",
    "#   print(list(train_ds.as_numpy_iterator()))\n",
    "#   type(test_ds)\n",
    "\n",
    "\n",
    "rmse_permute_df = pd.DataFrame(np.zeros((1,len(XCOLS))), columns=XCOLS)\n",
    "for fname in XCOLS:\n",
    "\n",
    "    # We're going to edit this data so make a deep copy of our preprocessed training dataset.\n",
    "    test_for_permute = test.copy(deep=True) \n",
    "    \n",
    "    # Approximate split permutation by simply reversing the data in this feature\n",
    "    test_for_permute[fname].values[:] = test_for_permute[fname].values[::-1]\n",
    "\n",
    "    # create TensorFlow BatchDataset\n",
    "    permute_ds = timeseries_dataset_from_df(test_for_permute, data_config[\"batch_size\"])\n",
    "\n",
    "    # evaluate model\n",
    "    rmse_permute_df[fname] = model.evaluate(permute_ds)**0.5\n",
    "\n",
    "    print('%s: %f rmse nano-Tesla' % (fname, rmse_permute_df[fname]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Relative Comparison: </b>Permutation Importance is evaluated as the influence a feature has  relative to our unpermuted baseline performance. It's typical to use either a ratio or subtraction to relate to our baseline.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "DG43wGSwETt7",
    "outputId": "36dd3f42-3a95-4517-bf03-8ef04150633c"
   },
   "outputs": [],
   "source": [
    "# Ratio the Permuted RMSE to the overall RMSE and sort in order of importance\n",
    "print('In order of most important feature first to least important by rmse(j)/rmse:')\n",
    "rmse_ratio_df = (rmse_permute_df/rmse).sort_values(ascending=False, by=0, axis=1)\n",
    "rmse_ratio_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Permutation Importance outcome\n",
    "plt.plot(rmse_ratio_df.columns, rmse_ratio_df.values.T, 'x-')\n",
    "plt.xticks(rotation=270)\n",
    "plt.ylabel('RMSE Ratio %')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Uncomment for a Pandas barplot with too many colors to easily interpret:\n",
    "#rmse_ratio_df.plot(kind='bar', figsize=(10, 5))\n",
    "#plt.title('Permutation Feature Importance')\n",
    "#plt.xlim(-0.25,)\n",
    "#plt.ylim(0.95, rmse_ratio_df.iloc[0,0])\n",
    "#plt.grid(True)\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLbZKYfgEjoT"
   },
   "source": [
    "### Review the feature Permutation Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions: </b>\n",
    "\n",
    "- How do our features compare in their influence on performance? \n",
    "    \n",
    "- How does the order of this list compare to your intuition from the Feature Correlation Heatmap we made earlier (aka heat map)?\n",
    "    \n",
    "- What are the model sensitivities to the input parameters? If one or more solar wind instruments were to degrade on orbit how do you predict that might impact model performance? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i>You can use this empty cell to write down your thoughts regarding the preceding question(s).</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWs2MeMe0ZFm"
   },
   "source": [
    "## Student Exercise: SHapley Additive exPlanations (SHAP)\n",
    "\n",
    "Use the SHapley Additive exPlanations (SHAP) by Lundberg and Lee (2017) methodology to explore the contribution of each feature on the predicted output labels for a specific event.\n",
    "\n",
    "Resources you might like to explore:\n",
    "*   [Interpreting your deep learning model by SHAP](https://towardsdatascience.com/interpreting-your-deep-learning-model-by-shap-e69be2b47893)\n",
    "*   [Interpretable Machine Learning by Christop Molnar](https://christophm.github.io/interpretable-ml-book/shap.html)\n",
    "*   [Keras LSTM for IMDB Sentiment Classification](https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/sentiment_analysis/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html?highlight=LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Optional Task: SHapley Additive exPlanations (SHAP)</b>\n",
    "    \n",
    "Use the SHapley Additive exPlanations (SHAP) by Lundberg and Lee (2017) methodology to explore the contribution of each feature on the predicted output labels for a specific event.\n",
    "\n",
    "Resources you might like to explore:\n",
    "\n",
    "- Interpreting your deep learning model by SHAP\n",
    "- Interpretable Machine Learning by Christop Molnar\n",
    "- Keras LSTM for IMDB Sentiment Classification    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acm3GxIUbeCL"
   },
   "source": [
    "## Note on SKT-Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_r6uovwbeCL"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "While the current version of [skt-explain](https://github.com/monte-flora/scikit-explain) module isn't compatible with our tabular LSTM model (and sister CNN notebook), you might like to monitor this module's progress for your various needs--they have some really wonderful XAI tools and visualizations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pA-g5jrAbeCL"
   },
   "source": [
    "# Event Case Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqWamOEKbeCL"
   },
   "source": [
    "## Define Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHpsrh-WbeCL"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "TIMESTEPS = data_config['timesteps']\n",
    "\n",
    "def predict_dst(\n",
    "    solar_wind_7d: pd.DataFrame,\n",
    "    satellite_positions_7d: pd.DataFrame,\n",
    "    latest_sunspot_number: float,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Take all of the data up until time t-1, and then make predictions for\n",
    "    times t and t+1.\n",
    "    Parameters\n",
    "    ----------\n",
    "    solar_wind_7d: pd.DataFrame\n",
    "        The last 7 days of satellite data up until (t - 1) minutes [exclusive of t]\n",
    "    satellite_positions_7d: pd.DataFrame\n",
    "        The last 7 days of satellite position data up until the present time [inclusive of t]\n",
    "    latest_sunspot_number: float\n",
    "        The latest monthly sunspot number (SSN) to be available\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : Tuple[float, float]\n",
    "        A tuple of two predictions, for (t and t + 1 hour) respectively; these should\n",
    "        be between -2,000 and 500.\n",
    "    \"\"\"\n",
    "    # Re-format data to fit into our pipeline\n",
    "    sunspots = pd.DataFrame(index=solar_wind_7d.index, columns=[\"smoothed_ssn\"])\n",
    "    sunspots[\"smoothed_ssn\"].values[:] = latest_sunspot_number\n",
    "    \n",
    "    # Process our features and grab last 32 (timesteps) hours\n",
    "    features, s = preprocess_features(\n",
    "        solar_wind_7d, sunspots, scaler=scaler, subset=SOLAR_WIND_FEATURES\n",
    "    )\n",
    "    model_input = features[-TIMESTEPS:][XCOLS].values.reshape(\n",
    "        (1, TIMESTEPS, features.shape[1])\n",
    "    )\n",
    "    #pprint.pprint(features)\n",
    "    \n",
    "    # Make a prediction\n",
    "    prediction_at_t0, prediction_at_t1 = model.predict(model_input)[0]\n",
    "\n",
    "    # Optional check for unexpected values\n",
    "    if not np.isfinite(prediction_at_t0):\n",
    "        prediction_at_t0 = -12\n",
    "    if not np.isfinite(prediction_at_t1):\n",
    "        prediction_at_t1 = -12\n",
    "\n",
    "    return prediction_at_t0, prediction_at_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02qv90XObeCM"
   },
   "source": [
    "## Ingest Real Event Data from Competition's \"Private\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0LXe_NxbeCM",
    "outputId": "25169f45-b6a1-4465-a187-494cd01f63d9"
   },
   "outputs": [],
   "source": [
    "# Real Event Data from PRIVATE\n",
    "DATA_PATH = Path(\"data/private/\")\n",
    "print('Importing data from: %s' % DATA_PATH)\n",
    "\n",
    "dst = pd.read_csv(DATA_PATH / \"dst_labels.csv\")\n",
    "dst.timedelta = pd.to_timedelta(dst.timedelta)\n",
    "dst.set_index([\"period\", \"timedelta\"], inplace=True)\n",
    "\n",
    "sunspots = pd.read_csv(DATA_PATH / \"sunspots.csv\")\n",
    "sunspots.timedelta = pd.to_timedelta(sunspots.timedelta)\n",
    "sunspots.set_index([\"period\", \"timedelta\"], inplace=True)\n",
    "\n",
    "solar_wind = pd.read_csv(DATA_PATH / \"solar_wind.csv\")\n",
    "solar_wind.timedelta = pd.to_timedelta(solar_wind.timedelta)\n",
    "solar_wind.set_index([\"period\", \"timedelta\"], inplace=True)\n",
    "\n",
    "satellite_positions = pd.read_csv(DATA_PATH / \"satellite_positions.csv\")\n",
    "satellite_positions.timedelta = pd.to_timedelta(satellite_positions.timedelta)\n",
    "satellite_positions.set_index([\"period\", \"timedelta\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0lreT8xbeCM"
   },
   "source": [
    "## Event: Geomagnetic storm with <i>Dst</i> minimum of approx. -180 nT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions: </b>\n",
    "\n",
    "- How does our LSTM model output for this event look compared to our observed <i>Dst</i>?\n",
    "- How does our LSTM model performance compare to the CNN notebook for this event?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i>You can use this empty cell to write down your thoughts regarding the preceding question(s).</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wR54LwzwbeCM",
    "outputId": "d708acb8-f8d8-4f94-8539-2694e6aaa828"
   },
   "outputs": [],
   "source": [
    "# Here we've chosen a specific event case study, you can choose different \n",
    "#      case studied by looking at the sibling notebook magnet_cnn_tutorial.ipynb\n",
    "event_start_day = 140\n",
    "idx_event_1day = range(event_start_day,       event_start_day       + 7      )\n",
    "idx_event_1hr  = range(event_start_day*24,    event_start_day*24    + 7*24   )\n",
    "idx_event_1min = range(event_start_day*24*60, event_start_day*24*60 + 7*24*60)\n",
    "\n",
    "dst_predicted_t0 = np.nan * np.zeros(len(idx_event_1hr))\n",
    "dst_predicted_t1 = np.nan * np.zeros(len(idx_event_1hr))\n",
    "i_dst = 0\n",
    "###idx_1min = range((event_start_day-7)*24*60, event_start_day*24*60)\n",
    "for i_offset_hour in range(-7*24, 0):\n",
    "    \n",
    "    # for the \n",
    "    idx_7day_1min = range(idx_event_1min[0]+i_offset_hour*60 - 1,  idx_event_1min[-1]+i_offset_hour*60 - 1)\n",
    "    \n",
    "    idx_7day_1day = range(idx_event_1day[0]+i_offset_hour//24, idx_event_1day[-1]+i_offset_hour//24)\n",
    "    \n",
    "    # Subset to 7 days around event\n",
    "    solar_wind_7d_by_min          = solar_wind.iloc[idx_7day_1min]\n",
    "    satellite_positions_7d_by_day = satellite_positions.iloc[idx_7day_1day]\n",
    "    # FIXME: This sunspot number is correct for day 140. Use DF.join to generalize lining up the correct value.\n",
    "    ###latest_sunspot_number = solar_wind_7d_by_min.join(sunspots).smoothed_ssn.mean()\n",
    "    latest_sunspot_number = sunspots.iloc[5] \n",
    "\n",
    "\n",
    "    # Predict Dst\n",
    "    dst_t0_t1 = predict_dst(solar_wind_7d=solar_wind_7d_by_min, satellite_positions_7d=satellite_positions_7d_by_day, latest_sunspot_number=latest_sunspot_number)\n",
    "\n",
    "    dst_predicted_t0[i_dst] = dst_t0_t1[0]\n",
    "    dst_predicted_t1[i_dst] = dst_t0_t1[1]\n",
    "\n",
    "    i_dst += 1\n",
    "    \n",
    "    # Uncomment to see the input and output data every hour:\n",
    "    #print('Hour %4d: SSN %.1f, Bz %.1f nT, V %.0f km/s, Dst [t0,t1] = [%.1f, %.1f] nT' \n",
    "    #    % (i_offset_hour, latest_sunspot_number, solar_wind_7d_by_min['bz_gsm'].mean(), \n",
    "    #    solar_wind_7d_by_min['speed'].mean(), dst_t0_t1[0], dst_t0_t1[1]))\n",
    "\n",
    "\n",
    "# Summarize final block of Input data\n",
    "print('\\nSummarizing final block of input data (head and tail):')\n",
    "pprint.pprint(solar_wind_7d_by_min['bz_gsm'].head())\n",
    "pprint.pprint(solar_wind_7d_by_min['bz_gsm'].tail())\n",
    "pprint.pprint(satellite_positions_7d_by_day['gse_x_ace'].head())\n",
    "pprint.pprint(satellite_positions_7d_by_day['gse_x_ace'].tail())\n",
    "pprint.pprint(latest_sunspot_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7N7RgNYVbeCN",
    "outputId": "abdccca2-0868-40c6-9367-28e283cedcac"
   },
   "outputs": [],
   "source": [
    "# RMSE for this event:\n",
    "#      Remember to line the indices up for observed Dst and predicted Dst[t1]\n",
    "rmse_t0 = np.mean((dst['dst'][idx_event_1hr]     - dst_predicted_t0     )**2)**0.5\n",
    "rmse_t1 = np.mean((dst['dst'][idx_event_1hr][1:] - dst_predicted_t1[:-1])**2)**0.5\n",
    "print('RMSE to t0 prediction: %f nT' % rmse_t0 )\n",
    "print('RMSE to t1 prediction: %f nT' % rmse_t1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "_sYxIvcXbeCN",
    "outputId": "494f3e37-0cc5-4018-c225-93ebc319bc52"
   },
   "outputs": [],
   "source": [
    "# Dst Observed\n",
    "ax = dst['dst'][idx_event_1hr].plot(title='Dst', rot=25, figsize=(15, 8), fontsize=14, label='Dst Observed')\n",
    "\n",
    "# Dst Predicted at t0 and t1\n",
    "#      Shift Dst[t1] to the right one hour to line up with the time axis\n",
    "ax.plot(dst_predicted_t0, 'b.', label='Dst Predicted t0')\n",
    "ax.plot(np.concatenate(([np.nan],dst_predicted_t1[:-1])), 'r.', label='Dst Predicted t1')\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "aXHnV7nubeCN",
    "outputId": "2593481b-a7d1-46d7-8c78-b9335375b4eb"
   },
   "outputs": [],
   "source": [
    "''' Uncomment this block if you want to plot just the Dst predictions.\n",
    "# Dst Predicted\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.plot(dst_predicted_t0, 'b', label='Dst Predicted t0')\n",
    "plt.plot(dst_predicted_t1, 'r', label='Dst Predicted t1')\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('Dst Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jn5uRW1kDq2p"
   },
   "source": [
    "## Student Exercise: Additional Case Studies and Degraded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Optional Task: </b>Look at the CNN notebook (magnet_cnn_tutorial.ipynb) and add additional events here and compare the LSTM and CNN performance for each event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions: </b>\n",
    "    \n",
    "- Are there storm events where LSTM is close to performing as well as the CNN model (see magnet_cnn_tutorial.ipynb)?\n",
    "- Are there phases or characteristics of different storm events where LSTM or CNN do better than each other?\n",
    "\n",
    "You can use these storm phase descriptions for contextualizing your findings:\n",
    "- Climatology / quiet periods: <i>Dst</i> is generally horizontal and nearly 0 nano-Tesla.\n",
    "- Sudden Impulse: <i>Dst</i> rises from near 0 to positive values rapidly over a few hours. \n",
    "- Storm Sudden Commencement and Main Phase: <i>Dst</i> drops sharply and remains significantly negative for up to several days.\n",
    "- Storm Peak: <i>Dst</i> reaches it's minmum (most negative) value.\n",
    "- Recovery Phase: <i>Dst</i> recovers from large negative values back to climatology, near 0 nano-Tesla.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i>You can use this empty cell to write down your thoughts regarding the preceding question(s).</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kEgSgWZbeCN"
   },
   "source": [
    "## Student Exercise: Degraded or Adverserial Data - How robust is the model?\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Optional Task: </b>Students: Here's your chance to degrade the instrument measurements and run the model to see how the performance is impacted. Start simple by adding Gaussian noise (mean 0), to the least important and the most important input parameters (aka features) and evaluating a specific event. You could also fold in the SHAP methodology here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions: </b>\n",
    "\n",
    "1. Thinking about your predictions from yesterday, if you degrade the data for one or more input parameters (features) by adding Gaussian noise, how does that impact the model performance to predict <i>Dst</i>?\n",
    "2. Based on what you learned in the lectures and your review of these case studies, what would you show your end user if they asked for a case study? How would you do this? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i>You can use this empty cell to write down your thoughts regarding the preceding question(s).</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpOzLqoyDSRA"
   },
   "source": [
    "## Disclaimer\n",
    "\n",
    "The United States Department of Commerce (DOC) GitHub project code is provided on an ‘as is’ basis and the user assumes responsibility for its use. DOC has relinquished control of the information and no longer has responsibility to protect the integrity, confidentiality, or availability of the information. Any claims against the Department of Commerce stemming from the use of its GitHub project will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise, does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "v003LoxwbeB6",
    "kRzCSuRwbeB7",
    "YIeZxbJXbeB7",
    "CdZDisojbeB8",
    "kJQsAZGlbeB8",
    "GCrRyAmibeB9",
    "5WEhRi6RMX0o",
    "AVRI8V_AI3NS",
    "ZDtowUzbMXNx",
    "Vk0h7YPGMxUw",
    "Is66dqN0M3VP",
    "5HoW8LxqM8eS",
    "J261z_FvOK_p",
    "HqKPO3qXOWDb",
    "_islsePpOgRI",
    "DHXdMwzzTHaF",
    "wkwxYtOqT8WT",
    "nl-bgU9WHXQO",
    "yLbZKYfgEjoT",
    "dWs2MeMe0ZFm",
    "acm3GxIUbeCL",
    "jn5uRW1kDq2p",
    "9kEgSgWZbeCN"
   ],
   "name": "MagNet_LSTM_with_XAI.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
