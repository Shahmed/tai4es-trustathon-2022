{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdacfac1",
   "metadata": {},
   "source": [
    "##  Notebook 00b: Tropical cyclone dataset exploration\n",
    "\n",
    "### Goal: Explore and evaluate the tropical cyclone dataset \n",
    "\n",
    "In this notebook,we will show how to load, explore, and prepare the data, as well as implement a first-pass deep learning model for predicting wind speeds.\n",
    "\n",
    "The training data for this competition consist of over 70,000 single-band images from storms in the Atlantic and East Pacific Oceans. The data come from a satellite band representing long-wave infrared frequency, which increases the brightness of clouds and should help us to better capture the spatial structure of storms. Each image has a corresponding wind speed contained in a separate file, measured in knots. For each image in the test set, we will predict the corresponding wind speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babe2d9",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf6d19",
   "metadata": {},
   "source": [
    "Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea337137",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53224133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd61a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where our downloaded images and metadata live locally\n",
    "DATA_PATH = Path.cwd().parent / \"data\" / \"final\" / \"public\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21059ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv(DATA_PATH / \"training_set_features.csv\")\n",
    "train_labels = pd.read_csv(DATA_PATH / \"training_set_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb59dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7352d9c8",
   "metadata": {},
   "source": [
    "The training data consist of images identified by a unique image_id. Each image_id is composed of {storm_id}_{image_number}, where storm_id is a unique three letter code and image_number represents the sequential ordering of images throughout that storm.\n",
    "\n",
    "Let's take a look at how many storms are in the training data, and how many images we have per storm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.storm_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4065cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_counts = train_metadata.groupby(\"storm_id\").size()\n",
    "storm_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d89723",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.hist(storm_counts, bins=50, color=\"lightgray\")\n",
    "plt.xlabel(\"Number of Images\")\n",
    "plt.ylabel(\"Number of Storms\")\n",
    "plt.title(\"Number of Images per Storm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1668b8",
   "metadata": {},
   "source": [
    "The training data contain images from 494 storms. We have anywhere from 4 to 648 images per unique storm, with many storms containing fewer than 100 images.\n",
    "\n",
    "We can also take a look at how many storms come from each ocean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.groupby(\"ocean\")[\"storm_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd561b",
   "metadata": {},
   "source": [
    "Next, let's explore the overall distribution of wind speeds in the training data by looking at train_labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdfacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475183c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "sns.boxplot(x=train_labels.wind_speed, color=\"lightgray\")\n",
    "plt.xlabel(\"Wind Speed\")\n",
    "plt.title(\"Distribution of Wind Speeds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b0888",
   "metadata": {},
   "source": [
    "Wind speeds in the training data range from 15 to 185 knots. A majority of images fall between 30 and 62 knots, but given what we know about storm intensity, it is especially important that we be able to accurately estimate the highest wind speeds.\n",
    "\n",
    "Given the temporal nature of storm and wind patterns, relative time will be an important correlate of wind speed. Our metadata contains a relative time field measured in seconds since the beginning of a storm. We can merge train_metadata with train_labels to see how wind speeds change over the course of a storm for a couple of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ae0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train_metadata with train_labels on the image_id field\n",
    "full_metadata = train_metadata.merge(train_labels, on=\"image_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wind_speeds(storm_id):\n",
    "    storm = full_metadata[full_metadata.storm_id == storm_id]\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.scatter(\"relative_time\", \"wind_speed\", data=storm, color=\"lightgray\")\n",
    "    plt.ticklabel_format(useOffset=False)\n",
    "    plt.ylabel(\"Wind Speed\")\n",
    "    plt.title(f\"Wind Speed over Relative Time: Storm {storm_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fe2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample two random storms from full_metadata\n",
    "for storm in full_metadata.storm_id.sample(2, random_state=40):\n",
    "    plot_wind_speeds(storm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e199f",
   "metadata": {},
   "source": [
    "For storm pvj, wind speeds begin around 40 knots, steadily increase to 120 knots, and then drop off again. Storm xhj displays a similar pattern, except that wind speeds hover around 70 knots before dropping off. While there appear to be short periods of time for which we do not have images available for these two storms, there is a clear relationship between relative time and wind speed that should be incorporated into our model.\n",
    "\n",
    "Keep in mind that the test set contains storms not represented in the training set, so our model needs to be able to generalize to storms it hasn't seen before. For storms represented in both the training and test sets, the test set images always temporally succeed the training set images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b92cc5",
   "metadata": {},
   "source": [
    "Images\n",
    "Next, we can begin exploring the image data. We'll add a file_name column to our training metadata, which will contain a Path object with the full path to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e81143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path style access for pandas\n",
    "import pandas_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata[\"file_name\"] = (\n",
    "    DATA_PATH / \"train\" / full_metadata.image_id.path.with_suffix(\".jpg\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1b1ba",
   "metadata": {},
   "source": [
    "Now we can take a look at some of the storm imagery!\n",
    "\n",
    "Remember that because the data are single-band infrared images captured by Geostationary Operational Environmental Satellites (GOES), pixel values represent heat energy in the infrared spectrum. Data for this competition come from band #13. We will be able to see objects, like weather clouds, based on their temperatures, but these representations are actually invisible to the human eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcb01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_at_wind_speed(wind_speed):\n",
    "    sample_img = full_metadata[full_metadata.wind_speed == wind_speed].file_name.iloc[0]\n",
    "    return Image(str(sample_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424423d",
   "metadata": {},
   "source": [
    "What does a relatively low wind speed image look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_at_wind_speed(wind_speed=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507b939",
   "metadata": {},
   "source": [
    "How about a relatively high wind speed image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec39d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_at_wind_speed(wind_speed=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb3ab8",
   "metadata": {},
   "source": [
    "Interesting! How about the image with the highest wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc8888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_at_wind_speed(wind_speed=185)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29dab2",
   "metadata": {},
   "source": [
    "With each increase in wind speed, we can see noticeable changes in storm structure and intensity. An effective model will be able to detect these types of patterns at scale.\n",
    "\n",
    "Finally, let's confirm that the first few training images are the expected size of 366 x 366 pixels using Pillow, imported as PIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c72621",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    pil_image.open(full_metadata.iloc[i][\"file_name\"]).convert(\"RGB\") for i in range(5)\n",
    "]\n",
    "for image in examples:\n",
    "    print(image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9644e",
   "metadata": {},
   "source": [
    "Split the Data\n",
    "\n",
    "The test set here includes a set of storms not included in the training data, as well as unseen imagery from later in a training storm's life cycle. We do not want to overstate our model's performance by overfitting to one or more storms. To be sure that our method is sufficiently generalizable, we will set aside a portion of the training imagery to validate the model during its development.\n",
    "\n",
    "Since observations in a time series are not independent, we cannot randomly subset the training data into training and validation sets. In other words, the characteristics of temporal data, such as trends and seasonality, require that we take time into account when splitting the data. From a real-world perspective, it will always be the case that we will use images and wind measurements taken earlier in a storm to estimate wind speeds when new imagery comes in.\n",
    "\n",
    "To account for the temporal relationship of the data up to the point of prediction and to ensure that we are not using images captured later in a storm to estimate prior wind speeds, we will subset the training data into training and validation sets using the relative_time field. For the purposes of this benchmark, we will holdout the last 20% of each storm's available images for our validation set.\n",
    "\n",
    "You are encouraged to further incorporate relative time into your own model and should consider the implications of autocorrelation when determining how best to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a temporary column for number of images per storm\n",
    "images_per_storm = full_metadata.groupby(\"storm_id\").size().to_frame(\"images_per_storm\")\n",
    "full_metadata = full_metadata.merge(images_per_storm, how=\"left\", on=\"storm_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d18670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each storm is sorted by relative time\n",
    "# Identify the final 20% of images per storm\n",
    "full_metadata[\"pct_of_storm\"] = (\n",
    "    full_metadata.groupby(\"storm_id\").cumcount() / full_metadata.images_per_storm\n",
    ")\n",
    "train = full_metadata[full_metadata.pct_of_storm < 0.8].drop(\n",
    "    [\"images_per_storm\", \"pct_of_storm\"], axis=1\n",
    ")\n",
    "val = full_metadata[full_metadata.pct_of_storm >= 0.8].drop(\n",
    "    [\"images_per_storm\", \"pct_of_storm\"], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba34b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm pct of images in the validation set is approximately 20%\n",
    "len(val) / len(full_metadata) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e35a93",
   "metadata": {},
   "source": [
    "For the purposes of this benchmark, we will use a randomly selected subset of 10% of our available data for training and validation. This adjustment will increase performance as we build our initial pipeline, but can be adjusted back for final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10% to increase performance\n",
    "train = train.sample(frac=0.1, replace=False, random_state=1)\n",
    "val = val.sample(frac=0.1, replace=False, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd89c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from labels\n",
    "x_train = train.drop(\"wind_speed\", axis=1)\n",
    "y_train = train.wind_speed\n",
    "\n",
    "x_val = val.drop(\"wind_speed\", axis=1)\n",
    "y_val = val.wind_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee91451",
   "metadata": {},
   "source": [
    "Build the Model \n",
    "\n",
    "The goal of our first pass is to build a relatively simple model that outputs wind speed predictions given single-band infrared imagery as input. Once we test this basic approach, we can add additional elements of sophistication and complexity, such as hyperparameter tuning or sequence modeling. We will use a lightweight PyTorch wrapper called PyTorch Lightning for this benchmark solution.\n",
    "\n",
    "Rather than train an entire Convolutional Neural Network (CNN) from scratch, we will fine-tune a pretrained model for transfer learning. This means that we will initialize our weights using a model that has been pretrained on a huge image dataset, replace and retrain its fully connected layer, and update the weights of the entire network by continuing backpropagation. There are many pretrained models to choose from. For this exercise, we will use a network called ResNet 152, which was prepared by Microsoft Research Asia in 2015 for the Large Scale Visual Recognition Challenge and is pretrained on the ImageNet dataset.\n",
    "\n",
    "First, we will need to read the training data into memory, convert the data to PyTorch tensors, and serve the data to our model in batches. Luckily, the PyTorch Dataset and DataLoader classes make implementing these complex tasks relatively straightforward. A Dataset object allows us to define custom methods for working with the data, and a DataLoader object parallelizes data loading. If you haven't worked with these classes before, we highly recommend this short tutorial.\n",
    "\n",
    "Our custom dataset will inherit an abstract class called torch.utils.data.Dataset and override the following methods:\n",
    "\n",
    "__len__(): returns the length of the dataset, measured as number of samples\n",
    "__getitem__(): provided an index, returns a sample from the dataset\n",
    "The dataset object will return samples as dictionaries with keys for:\n",
    "\n",
    "image_id: the image id\n",
    "image: the image tensor\n",
    "label: the label, if it exists\n",
    "Since resnet152 was trained using images smaller than 366 x 366 pixels, we will use the torchvision.transforms.Compose module to downsize our images using center cropping, convert them to PyTorch tensors, and normalize their pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87410fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba870d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWIND(Dataset):\n",
    "    \"\"\"Reads in an image, transforms pixel values, and serves\n",
    "    a dictionary containing the image id, image tensors, and label.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_train, y_train=None):\n",
    "        self.data = x_train\n",
    "        self.label = y_train\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.CenterCrop(128),\n",
    "                transforms.ToTensor(),\n",
    "                # All models expect the same normalization mean & std\n",
    "                # https://pytorch.org/docs/stable/torchvision/models.html\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = pil_image.open(self.data.iloc[index][\"file_name\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        image_id = self.data.iloc[index][\"image_id\"]\n",
    "        if self.label is not None:\n",
    "            label = self.label.iloc[index]\n",
    "            sample = {\"image_id\": image_id, \"image\": image, \"label\": label}\n",
    "        else:\n",
    "            sample = {\n",
    "                \"image_id\": image_id,\n",
    "                \"image\": image,\n",
    "            }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5f0f6",
   "metadata": {},
   "source": [
    "Now that we have a way of processing the data, we can use torch.utils.data.DataLoader to serve the data within our model class (more on this later).\n",
    "\n",
    "Next, we will create a custom class to define our loss function, Root Mean Square Error (RMSE). As a reminder, RMSE represents the square root of the mean of the squared differences between the predicted values and the actual values. This class will inherit nn.Module, which is the base class for all neural network modules in PyTorch. The forward method is built upon MSELoss(), which measures mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    \"\"\"Measures root mean square error.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        return torch.sqrt(self.mse(pred, true))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba0ca1",
   "metadata": {},
   "source": [
    "Given our new DatasetWIND and RMSELoss classes, PyTorch Lightning will allow us to train a model using minimal for loops. This library keeps the flexibility of PyTorch but removes most of the boilerplate training code, making it less error prone, cleaner to read, and easier to update. By subclassing pl.LightningModule, most of the training logic will happen for us behind the scenes. If you're new to PyTorch Lightning, you may find their quick start guide helpful.\n",
    "\n",
    "We will define the following methods within our model:\n",
    "\n",
    "prepare_model: import a pretrained model and reinitialize the final layer with a new sequence of modules\n",
    "forward: define the forward pass for an image\n",
    "training_step (required): switch the model to train mode, implement the forward pass, and calculate training loss for a batch\n",
    "validation_step: switch the model to eval mode and calculate validation loss for a batch\n",
    "train_dataloader (required): call an iterable over the training dataset for automatic batching\n",
    "val_dataloader: call an iterable over the validation dataset for automatic batching\n",
    "configure_optimizers (required): configure an optimizer (we will use Adam); this step automatically calls backward and step in each epoch\n",
    "training_epoch_end: calculate the average loss for an epoch with the outputs of training_step\n",
    "validation_epoch_end: calculate the average loss for an epoch with the outputs of validation_step\n",
    "We'll also add a couple additional, optional, helper methods, e.g., fit to instantiate and fit a pl.Trainer object for training automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8acab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ec418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedWindModel(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(PretrainedWindModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.learning_rate = self.hparams.get(\"lr\", 2e-4)\n",
    "        self.hidden_size = self.hparams.get(\"embedding_dim\", 50)\n",
    "        self.dropout = self.hparams.get(\"dropout\", 0.1)\n",
    "        self.max_epochs = self.hparams.get(\"max_epochs\", 1)\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 0)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 10)\n",
    "        self.x_train = self.hparams.get(\"x_train\")\n",
    "        self.y_train = self.hparams.get(\"y_train\")\n",
    "        self.x_val = self.hparams.get(\"x_val\")\n",
    "        self.y_val = self.hparams.get(\"y_val\")\n",
    "        self.num_outputs = 1  # One prediction for regression\n",
    "\n",
    "        # Where final model will be saved\n",
    "        self.output_path = Path.cwd() / self.hparams.get(\"output_path\", \"model-outputs\")\n",
    "        self.output_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # Where TensorBoard logs will be saved\n",
    "        self.log_path = Path.cwd() / self.hparams.get(\"log_path\", \"logs\")\n",
    "        self.log_path.mkdir(exist_ok=True)\n",
    "        self.logger = pl.loggers.TensorBoardLogger(\n",
    "            self.log_path, name=\"benchmark_model\"\n",
    "        )\n",
    "\n",
    "        # Instantiate training and validation datasets\n",
    "        self.train_dataset = DatasetWIND(self.x_train, self.y_train)\n",
    "        self.val_dataset = DatasetWIND(self.x_val, self.y_val)\n",
    "        self.model = self.prepare_model()\n",
    "\n",
    "    def prepare_model(self):\n",
    "        res_model = models.resnet152(pretrained=True)\n",
    "        # Input size of 2048 for resnet152\n",
    "        # https://pytorch.org/hub/pytorch_vision_resnet/\n",
    "        res_model.fc = nn.Sequential(\n",
    "            nn.Linear(2048, self.hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_size, self.num_outputs),\n",
    "        )\n",
    "        return res_model\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"label\"]\n",
    "        criterion = RMSELoss()\n",
    "        # Switch to training mode\n",
    "        loss = criterion(\n",
    "            self.model.train().forward(x).squeeze(), y.type(torch.FloatTensor)\n",
    "        )\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"label\"]\n",
    "        criterion = RMSELoss()\n",
    "        # Switch to evaluation mode\n",
    "        loss = criterion(\n",
    "            self.model.eval().forward(x).squeeze(), y.type(torch.FloatTensor)\n",
    "        )\n",
    "        tensorboard_logs = {\"val_loss\": loss}\n",
    "        return {\"batch_val_loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, num_workers=self.num_workers, batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, num_workers=self.num_workers, batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack(tuple(output[\"loss\"] for output in outputs)).mean()\n",
    "        return {\n",
    "            \"avg_epoch_train_loss\": avg_train_loss,\n",
    "            \"progress_bar\": {\"avg_epoch_train_loss\": avg_train_loss},\n",
    "            \"log\": {\"avg_epoch_train_loss\": avg_train_loss},\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_val_loss = torch.stack(\n",
    "            tuple(output[\"batch_val_loss\"] for output in outputs)\n",
    "        ).mean()\n",
    "        return {\n",
    "            \"avg_epoch_val_loss\": avg_val_loss,\n",
    "            \"progress_bar\": {\"avg_epoch_val_loss\": avg_val_loss},\n",
    "            \"log\": {\"avg_epoch_val_loss\": avg_val_loss},\n",
    "        }\n",
    "\n",
    "    ## Convenience Methods ##\n",
    "\n",
    "    def fit(self):\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            default_root_dir=self.output_path,\n",
    "            logger=self.logger,\n",
    "            checkpoint_callback=pl.callbacks.ModelCheckpoint(\n",
    "                filepath=self.output_path,\n",
    "                monitor=\"avg_epoch_val_loss\",\n",
    "                mode=\"min\",\n",
    "                verbose=True,\n",
    "            ),\n",
    "            gradient_clip_val=self.hparams.get(\"gradient_clip_val\", 1),\n",
    "            num_sanity_val_steps=self.hparams.get(\"val_sanity_checks\", 0),\n",
    "        )\n",
    "        self.trainer.fit(self)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def make_submission_frame(self, x_test):\n",
    "        test_dataset = DatasetWIND(x_test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, num_workers=self.num_workers, batch_size=self.batch_size\n",
    "        )\n",
    "        submission_frame = pd.DataFrame(index=x_test.image_id, columns=[\"wind_speed\"])\n",
    "        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            x = batch[\"image\"]\n",
    "            preds = self.eval().forward(x)\n",
    "            submission_frame.loc[batch[\"image_id\"], \"wind_speed\"] = (\n",
    "                preds.detach().numpy().squeeze()\n",
    "            )\n",
    "        submission_frame.wind_speed = submission_frame.wind_speed.astype(float)\n",
    "        return submission_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584aa96",
   "metadata": {},
   "source": [
    "Fit the Model \n",
    "\n",
    "Finally, it's time to fit our model. A PretrainedWindModel can be instantiated using only a dictionary of hparams. The only required hparams are the training and validation data, but there are several additional hyperparameters we can specify to explore modeling strategies, including the learning rate, dropout rate, and hidden layer size. Consider experimenting with different models and using hyperparameter tuning to find the best combination of parameters to increase performance.\n",
    "\n",
    "Once we specify our hparams, we can simply call the fit method to begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # Required hparams\n",
    "    \"x_train\": x_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"x_val\": x_val,\n",
    "    \"y_val\": y_val,\n",
    "    # Optional hparams\n",
    "    \"lr\": 2e-4,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"dropout\": 0.1,\n",
    "    \"max_epochs\": 4,\n",
    "    \"batch_size\": 10,\n",
    "    \"num_workers\": 0,\n",
    "    \"gradient_clip_val\": 1,\n",
    "    \"val_sanity_checks\": 0,\n",
    "    \"output_path\": \"model-outputs\",\n",
    "    \"log_path\": \"logs\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe7fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_model = PretrainedWindModel(hparams=hparams)\n",
    "storm_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06be7e7",
   "metadata": {},
   "source": [
    "Note: PyTorch Lightning lets you log PyTorch models and metrics into a directory for visualization within the TensorBoard UI. TensorBoard is a machine learning visualization toolkit that's helpful for tracking metrics across batches, epochs, and models. You can install TensorBoard through the command line to visualize your logged data by specifying the root log directory and running tensorboard --logdir=logs/benchmark_model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb450db",
   "metadata": {},
   "source": [
    "Make a Submission\n",
    "Now that our model is trained, we are finally ready to perform inference and make a submission. You'll only want to perform inference on the test set once you determine your top performing model, to avoid inadvertently overfitting.\n",
    "\n",
    "First, let's take a look at the submission format. Make sure to that your submission sets image_id as the index to avoid getting a submission error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8923332",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv(\n",
    "    DATA_PATH / \"submission_format.csv\", index_col=\"image_id\"\n",
    ")\n",
    "submission_format.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f52bc1",
   "metadata": {},
   "source": [
    "PyTorch Lightning automatically saves everything you need to restore training sessions as checkpoints in your current working directory. We updated this directory using the output_path hyperparameter in hparams. Even though model training has the potential to get interrupted, you should always be able to pick back up where you left off without having to restart training from scratch.\n",
    "\n",
    "Let's load our best model from the model-outputs directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74585ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best checkpoint based on logs\n",
    "best_checkpoint = str(Path(\"model-outputs\") / \"epoch=3.ckpt\")\n",
    "example_model = PretrainedWindModel.load_from_checkpoint(best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ad9a7",
   "metadata": {},
   "source": [
    "Finally, we can estimate wind speeds in the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metadata = pd.read_csv(DATA_PATH / \"test_set_features.csv\")\n",
    "test_metadata[\"file_name\"] = (\n",
    "    DATA_PATH / \"test\" / test_metadata.image_id.path.with_suffix(\".jpg\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72dc108",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = example_model.make_submission_frame(test_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5aa8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure submission indices match submission format\n",
    "assert submission_format.index.equals(submission.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d909611",
   "metadata": {},
   "source": [
    "The first few predictions look reasonable, based on our training data exploration. However, wind_speed must be an integer based on the submission_format, so let's round our final predictions to the nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.round().astype(int)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125bca0",
   "metadata": {},
   "source": [
    "Let's take a look at the range of wind speeds that we are estimating in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bcac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.wind_speed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9414f09",
   "metadata": {},
   "source": [
    "A minimum wind speed of 24 knots and a maximum wind speed of 153 knots seems like a reasonable range.\n",
    "\n",
    "Finally, we will save our submission locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv((DATA_PATH / \"submission.csv\"), index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
